{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "Project: **The price of being late: short- and long-term consequences of a delayed migration timing**  \n",
    "Author: Iris Bontekoe  \n",
    "Program: Python 3.8.5  \n",
    "Description: This script prepares the data for analyses and figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff that is necessary to execute the script\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "from collections import namedtuple\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from statistics import mean\n",
    "\n",
    "# Set the path to the folder where the data is located\n",
    "data_folder = \"[...]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GPS data for calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine data for each study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Write a function to make sure all objects are removed afterwards\n",
    "def function_Aff():\n",
    "\n",
    "    # Find all file names for Affenberg\n",
    "    all_files_Aff = glob.glob(data_folder+\"TempWind/White Stork Affenberg*.csv\") + glob.glob(data_folder+\"TempWind/*.csv-*.csv\")\n",
    "\n",
    "    # Load all data files for Affenberg\n",
    "    data_Aff_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files_Aff)\n",
    "\n",
    "    # Merge the files together\n",
    "    data_Aff = pd.concat(data_Aff_all)\n",
    "\n",
    "    # Save the data into a new pkl file\n",
    "    data_Aff.to_pickle(data_folder+\"DataAff_TempWind_AllData.pkl\")\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_Aff()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Write a function to make sure all objects are removed afterwards\n",
    "def function_CC():\n",
    "    \n",
    "    # Find all file names for CareCenter\n",
    "    all_files_CC = glob.glob(data_folder+\"TempWind/*White Stork SW Germany Care Centre*.csv\")\n",
    "\n",
    "    # Load all data files for CareCenter\n",
    "    data_CC_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files_CC)\n",
    "\n",
    "    # Merge the files together\n",
    "    data_CC = pd.concat(data_CC_all)\n",
    "\n",
    "    # Save the data into a new pkl file\n",
    "    data_CC.to_pickle(data_folder+\"DataCC_TempWind_AllData.pkl\")\n",
    "    \n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_CC()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Write a function to make sure all objects are removed afterwards\n",
    "def function_CASCB():\n",
    "\n",
    "    # Find all file names for CASCB\n",
    "    all_files_CASCB = glob.glob(data_folder+\"TempWind/*White Stork SW Germany CASCB*.csv\")\n",
    "\n",
    "    # Load all data files for CASCB\n",
    "    data_CASCB_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files_CASCB)\n",
    "\n",
    "    # Merge the files together\n",
    "    data_CASCB = pd.concat(data_CASCB_all)\n",
    "\n",
    "    # Save the data into a new pkl file\n",
    "    data_CASCB.to_pickle(data_folder+\"DataCASCB_TempWind_AllData.pkl\")\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_CASCB()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "- Remove unnecessary columns\n",
    "- Remove duplicates\n",
    "- Remove data before release (if applicable)\n",
    "- Remove data after death (if applicable)\n",
    "- Remove data from 1 November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of columns to keep\n",
    "columns_to_keep = [\n",
    "    \"timestamp\",\n",
    "    \"location-long\",\n",
    "    \"location-lat\",\n",
    "    \"tag-local-identifier\",\n",
    "    \"individual-local-identifier\",\n",
    "    \"ground-speed\",\n",
    "    \"heading\",\n",
    "    \"height-above-ellipsoid\",\n",
    "    \"sensor-type\",\n",
    "    \"gps:satellite-count\",\n",
    "    \"Temp_SL\",\n",
    "    \"U_Wind_PL\",\n",
    "    \"V_Wind_PL\",\n",
    "]\n",
    "\n",
    "# Load the release-death data\n",
    "ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def function_prep(Released):\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Change some column names\n",
    "    data.rename({\n",
    "        \"ECMWF ERA5 SL Temperature (2 m above Ground)\":\"Temp_SL\",\n",
    "        \"ECMWF ERA5 PL U Wind\":\"U_Wind_PL\",\n",
    "        \"ECMWF ERA5 PL V wind\":\"V_Wind_PL\"\n",
    "    }, axis=1, inplace=True)\n",
    "\n",
    "    # Only keep columns that are necessary for further steps in the analyses\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Only keep GPS data\n",
    "    data = data[data[\"sensor-type\"]==\"gps\"]\n",
    "\n",
    "    # Only keep locations that are not NA\n",
    "    data.dropna(subset=[\"location-long\",\"location-lat\"],inplace=True)\n",
    "\n",
    "    # Convert the timestamps\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "\n",
    "    # Remove data before release and after death\n",
    "    \n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "   \n",
    "    # Remove duplicates\n",
    "\n",
    "    # Sort the data\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\",\"gps:satellite-count\"], ascending=[True,True,True], inplace=True)\n",
    "\n",
    "    # Remove duplicated timestamps\n",
    "    data.drop_duplicates([\"timestamp\",\"tag-local-identifier\"],keep=\"first\",inplace=True)\n",
    "\n",
    "    # Add the start-year of the data\n",
    "    Start_year = data.groupby(\"tag-local-identifier\")[\"timestamp\"].min().dt.year.reset_index()\n",
    "    data[\"StartYear\"] = data[\"tag-local-identifier\"].map(Start_year.set_index(\"tag-local-identifier\")[\"timestamp\"].to_dict())\n",
    "    \n",
    "    # Remove data from 1 November in the year of tagging\n",
    "    data = data[data[\"timestamp\"]<[datetime.datetime(y,11,1) for y in data[\"StartYear\"]]]\n",
    "    \n",
    "    # Add the aviary\n",
    "    data[\"Aviary\"] = Aviary\n",
    "    \n",
    "    # Add an individual ID\n",
    "    data[\"Individual\"] = data[\"Aviary\"].map(str)+\"_\"+data[\"tag-local-identifier\"].map(str)\n",
    "    \n",
    "    # Add Day without time\n",
    "    data[\"Day\"] = data[\"timestamp\"].dt.date\n",
    "    \n",
    "    # Save the data into a new pkl file\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_AllData.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_P.pkl\"\n",
    "aviary = [\"Affenberg_2019\",\"Affenberg_2020\"]\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_AllData.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_P.pkl\"\n",
    "aviary = [\"CareCenter_2019\",\"CareCenter_2020\"]\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_AllData.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_P.pkl\"\n",
    "aviary = [\"CASCB_East\",\"CASCB_West\"]\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep(Released=False)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add daylength\n",
    "\n",
    "- Daylenght per day in minutes\n",
    "- Sunrise for the first location of the day\n",
    "- Sunset for the last location of the day\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def DayLength():\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Order the data frame by individual and timestamp\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\"], ascending=[True,True], inplace=True)\n",
    "    \n",
    "    # Add a new column for daylength\n",
    "    data[\"DayLength\"] = np.nan\n",
    "    \n",
    "    for i in data[\"tag-local-identifier\"].unique():\n",
    "\n",
    "        for Day in data[\"Day\"].unique():\n",
    "\n",
    "            if len(data[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"]==Day)])<1:\n",
    "                continue\n",
    "                \n",
    "            # Save the first and last location of the day\n",
    "            FirstLoc = LocationInfo('name', 'region', 'timezone/name', data[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"]==Day)][\"location-lat\"].iloc[0], data[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"]==Day)][\"location-long\"].iloc[0])\n",
    "            LastLoc = LocationInfo('name', 'region', 'timezone/name', data[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"]==Day)][\"location-lat\"].iloc[-1], data[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"]==Day)][\"location-long\"].iloc[-1])\n",
    "\n",
    "            # Calculate the time of sunset and sunrise and add the difference in minutes to the data\n",
    "            data.loc[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"]==Day),\"DayLength\"] = (sun(LastLoc.observer, date=Day)[\"sunset\"]-sun(FirstLoc.observer, date=Day)[\"sunrise\"]).total_seconds()/60\n",
    "\n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_P.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_P.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DayLength()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_P.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_P.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DayLength()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_P.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_P.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DayLength()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split bursts and assign IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function nearest\n",
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function SplitGPSBursts\n",
    "def SplitGPSBursts(MaxTimeDiff=10,MinBurstLength=120):\n",
    "    \n",
    "    #---------------------------------#\n",
    "    #- Preparation of the data frame -#\n",
    "    #---------------------------------#\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Sort the data frame by individual and date\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\"], ascending=[True,True], inplace=True)\n",
    "    \n",
    "    # Reset indices\n",
    "    data.reset_index(inplace=True)\n",
    "    \n",
    "    #----------------------#\n",
    "    #- Identifying bursts -#\n",
    "    #----------------------#\n",
    "\n",
    "    # Find which rows of the dataframe differ more than MaxTimeDiff from the previous row\n",
    "    #    Index-numbers of these rows will be saved in Breaks\n",
    "    #    Note: If the tag takes a burst, a position is taken every second, so the time difference between\n",
    "    #        locations in the same burst is 1 second. A break of at least 10 seconds to be counted as separte bursts\n",
    "   \n",
    "    Breaks = list(data[data[\"timestamp\"].diff().dt.total_seconds()>MaxTimeDiff].index)\n",
    "    \n",
    "    # Include 0 at the start\n",
    "    Breaks.insert(0, 0)\n",
    "        \n",
    "    # Include the last index of the data at the end\n",
    "    Breaks.insert(len(Breaks), max(data.index))\n",
    "    \n",
    "    #-------------------#\n",
    "    #- Separate bursts -#\n",
    "    #-------------------#\n",
    "    \n",
    "    # Create a list with the length of data to enter a number for every separate 'burst'\n",
    "    BurstNo = [None] * len(data)\n",
    "    \n",
    "    # Create a list with the length of data to enter if the data belongs to an actual burst (at least MinBurstLength long)\n",
    "    BelongsToBurst = [None] * len(data)\n",
    "\n",
    "    # Add the number for every burst\n",
    "    for i in range(len(Breaks)-1):\n",
    "        BurstNo[Breaks[i]:Breaks[i+1]] = [i]*len(BurstNo[Breaks[i]:Breaks[i+1]])\n",
    "        \n",
    "        if len(BurstNo[Breaks[i]:Breaks[i+1]]) >= MinBurstLength:\n",
    "            BelongsToBurst[Breaks[i]:Breaks[i+1]] = [True]*len(BurstNo[Breaks[i]:Breaks[i+1]])\n",
    "        else:\n",
    "            BelongsToBurst[Breaks[i]:Breaks[i+1]] = [False]*len(BurstNo[Breaks[i]:Breaks[i+1]])\n",
    "            \n",
    "        # The code above doesn't apply to the last item in the list (has to do with the way indexing works in Python-lists)\n",
    "        if Breaks[i+1] == (len(data)-1):\n",
    "            BurstNo[Breaks[i]:(Breaks[i+1]+1)] = [i]*len(BurstNo[Breaks[i]:(Breaks[i+1]+1)])\n",
    "            \n",
    "            if len(BurstNo[Breaks[i]:(Breaks[i+1]+1)]) >= MinBurstLength:\n",
    "                BelongsToBurst[Breaks[i]:(Breaks[i+1]+1)] = [True]*len(BurstNo[Breaks[i]:(Breaks[i+1]+1)])\n",
    "            else:\n",
    "                BelongsToBurst[Breaks[i]:(Breaks[i+1]+1)] = [False]*len(BurstNo[Breaks[i]:(Breaks[i+1]+1)])\n",
    "\n",
    "    \n",
    "    # Add the lists to the data\n",
    "    data[\"BurstID\"] = BurstNo\n",
    "    data[\"BelongsToBurst\"] = BelongsToBurst\n",
    "    \n",
    "    #--------------------------#\n",
    "    #- Assign burst to a time -#\n",
    "    #--------------------------#\n",
    "\n",
    "    # Make lists with the time on wich bursts should have started\n",
    "    StartTime1 = pd.date_range(data[\"Day\"].min(),data[\"Day\"].max()+datetime.timedelta(days=1), freq=\"15min\")\n",
    "    StartTime2 = pd.date_range(datetime.datetime.combine(data[\"Day\"].min(),datetime.time(10, 40)),datetime.datetime.combine(data[\"Day\"].max(),datetime.time(10, 40))+datetime.timedelta(days=1), freq=\"D\")\n",
    "    StartTime = StartTime1.union(StartTime2) # Is already sorted\n",
    "    \n",
    "    # Add a new column to data\n",
    "    data[\"Burst_A\"] = np.nan\n",
    "    \n",
    "    # Find the nearest start time for every burst and enter the start time in the new column\n",
    "    for i in data[data[\"BelongsToBurst\"]==True][\"BurstID\"].unique():\n",
    "        First_timestamp = data[data[\"BurstID\"]==i][\"timestamp\"].min()\n",
    "        data.loc[data[\"BurstID\"]==i,\"Burst_A\"] = nearest(items=StartTime,pivot=First_timestamp)\n",
    "\n",
    "    #-----------------#\n",
    "    #- Save the data -#\n",
    "    #-----------------#\n",
    "    \n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_P.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SplitGPSBursts()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_P.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SplitGPSBursts()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_P.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SplitGPSBursts()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match bursts between individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function FindOverlappingGPSBursts\n",
    "\n",
    "def FindOverlappingGPSBursts(MinNumOverlap=100,MaxDist=1):\n",
    "    #---------------------------------#\n",
    "    #- Preparation of the data frame -#\n",
    "    #---------------------------------#\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Sort the data frame by individual and date\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\",\"Burst_A\"], ascending=[True,True,True], inplace=True)\n",
    "    \n",
    "    # Reset indices\n",
    "    data.reset_index(inplace=True)\n",
    "    \n",
    "    # Make a new column in the data frame in which the BurstNumber will be entered\n",
    "    data[\"Burst\"] = np.nan\n",
    "    \n",
    "    #--------------#\n",
    "    #- For-loop 1 -#\n",
    "    #--------------#\n",
    "\n",
    "    # Make a list with all different values of Burst_A.\n",
    "    #    Bursts with the same value for Burst_A were matched to the same time\n",
    "    List_of_Bursts = data[\"Burst_A\"].dropna().unique()\n",
    "\n",
    "    # Loop through the following instructions for every burst in list_of_Bursts\n",
    "    for Burst in List_of_Bursts:\n",
    "        \n",
    "        # Find all BurstIDs within Burst\n",
    "        BurstIDs = data[data[\"Burst_A\"] == Burst][\"BurstID\"].unique()\n",
    "        \n",
    "        #--------------#\n",
    "        #- For-loop 2 -#\n",
    "        #--------------#\n",
    "\n",
    "        # Loop through the following instructions for every value in IndividualBurstIDs\n",
    "        for ID1 in BurstIDs: # Start of for-loop 2\n",
    "            \n",
    "            #----------------------#\n",
    "            #- Time-saving checks -#\n",
    "            #----------------------#\n",
    "            \n",
    "            # Go to the start of the loop if the BurstID is already part of a burst\n",
    "            if len(data[data[\"BurstID\"] == ID1][\"Burst\"].dropna()) > 0:\n",
    "                continue\n",
    "            \n",
    "            #----------------------#\n",
    "            #- Define BurstNumber -#\n",
    "            #----------------------#\n",
    "            \n",
    "            # The BurstNumber of the currently handled burst should be one higher than the\n",
    "            #    maximum BurstNumber that is already in the data frame\n",
    "            if len(data[\"Burst\"].dropna()) > 0:\n",
    "                BurstNumber = data[\"Burst\"].dropna().max()+1\n",
    "                \n",
    "            else:\n",
    "                BurstNumber = 0\n",
    "\n",
    "            # Assign BurstNumber to the column Burst in the data\n",
    "            data.loc[data[\"BurstID\"] == ID1,\"Burst\"] = BurstNumber\n",
    "            \n",
    "            #--------------#\n",
    "            #- For-loop 3 -#\n",
    "            #--------------#\n",
    "            \n",
    "            for ID2 in BurstIDs: # Start of for-loop 3\n",
    "                \n",
    "                #----------------------#\n",
    "                #- Time-saving checks -#\n",
    "                #----------------------#\n",
    "            \n",
    "                # Go to the start of the loop if the BurstID is already part of a burst\n",
    "                if len(data[data[\"BurstID\"] == ID2][\"Burst\"].dropna()) > 0:\n",
    "                    continue\n",
    "                \n",
    "                # Go to the start of the loop if ID1 == ID2\n",
    "                if ID1 == ID2:\n",
    "                    continue\n",
    "                \n",
    "                # Go to the start of the loop if the overlap is less than MinNumOverlap\n",
    "                \n",
    "                # Define 'Range'\n",
    "                Range = namedtuple('Range', ['start', 'end'])\n",
    "\n",
    "                # Get the ranges for both BurstIDs\n",
    "                R_ID1 = Range(start=data[data[\"BurstID\"] == ID1][\"timestamp\"].iloc()[0],end=data[data[\"BurstID\"] == ID1][\"timestamp\"].iloc()[-1])\n",
    "                R_ID2 = Range(start=data[data[\"BurstID\"] == ID2][\"timestamp\"].iloc()[0],end=data[data[\"BurstID\"] == ID2][\"timestamp\"].iloc()[-1])\n",
    "\n",
    "                # Determine the latest start and earliest end for the ranges\n",
    "                latest_start = max(R_ID1.start,R_ID2.start)\n",
    "                earliest_end = min(R_ID1.end,R_ID2.end)\n",
    "                \n",
    "                # Calculate the difference between the start and end calculated above\n",
    "                delta = (earliest_end - latest_start).total_seconds() + 1\n",
    "\n",
    "                # Determine the overlap\n",
    "                overlap = max(0,delta)\n",
    "                \n",
    "                if overlap < MinNumOverlap:\n",
    "                    continue\n",
    "                                    \n",
    "                #-------------------#\n",
    "                #- Check locations -#\n",
    "                #-------------------#\n",
    "                \n",
    "                # Make a subset data frame\n",
    "                data_dist = pd.merge(data[data[\"BurstID\"] == ID1][[\"timestamp\",\"location-lat\",\"location-long\"]], data[data[\"BurstID\"] == ID2][[\"timestamp\",\"location-lat\",\"location-long\"]], on=\"timestamp\")\n",
    "                \n",
    "                # Define a list to enter all distances\n",
    "                Distances = [None] * len(data_dist)\n",
    "\n",
    "                # Calculate the distance between the locations for each row in the data\n",
    "                for row in range(len(data_dist)):\n",
    "                    Distances[row] = geopy.distance.distance((data_dist[\"location-lat_x\"].iloc()[row], data_dist[\"location-long_x\"].iloc()[row]),(data_dist[\"location-lat_y\"].iloc()[row], data_dist[\"location-long_y\"].iloc()[row])).km\n",
    "\n",
    "                # Calculate the minimum and maximum distance\n",
    "                Min_Dist = min(Distances)\n",
    "                Max_Dist = max(Distances)\n",
    "                \n",
    "                # If the minimum distance is lower than MaxDist and the maximum distance is lower than 10 times MaxDist, enter the same BurstNumber in the column Burst in data\n",
    "                if Min_Dist < MaxDist and Max_Dist < 10*MaxDist:\n",
    "                    data.loc[data[\"BurstID\"] == ID2,\"Burst\"] = BurstNumber\n",
    "\n",
    "                    \n",
    "    #-------------#\n",
    "    #- Save data -#\n",
    "    #-------------#\n",
    "    \n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindOverlappingGPSBursts()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindOverlappingGPSBursts()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindOverlappingGPSBursts()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify flight, climbing and gliding segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function FlightClassification that calculates climbing rates and classifies flight, climbing and gliding segments\n",
    "def FlightClassification(MinGroundSpeed=2.5,RunningWindowLength=15,MinFlightTime=15,MinNonFlightTime=5,MinClimbingRate=0.2,MaxDecliningRate=0):\n",
    "\n",
    "    #---------------------------------#\n",
    "    #- Preparation of the data frame -#\n",
    "    #---------------------------------#\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Sort the data frame by individual and date\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\",\"BurstID\"], ascending=[True,True,True], inplace=True)\n",
    "    \n",
    "    # Reset indices\n",
    "    #data.reset_index(inplace=True)\n",
    "    \n",
    "    # Make a list with all BurstIDs\n",
    "    BurstIDs = data[\"BurstID\"].unique()\n",
    "    \n",
    "    #----------------------------#\n",
    "    #- Calculate climbing rates -#\n",
    "    #----------------------------#\n",
    "    \n",
    "    # Make new columns to enter the values into\n",
    "    data[\"ClimbingRate\"] = np.nan\n",
    "    \n",
    "    TimeDiff = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "    HeightDiff = data[\"height-above-ellipsoid\"].diff()\n",
    "    ClimbingRate = (HeightDiff/TimeDiff).tolist()\n",
    "    \n",
    "    # Enter climbing rates into the data frame, only within bursts\n",
    "\n",
    "    # Make a list that indicates if a data point belongs to a burst\n",
    "    InBurst = [i for i, x in enumerate(data[\"BurstID\"].diff()) if x == 0]\n",
    "    \n",
    "    # Enter ClimbingRate into the data, but only for the rows belonging to a burst\n",
    "    data.loc[data.index.isin(InBurst),\"ClimbingRate\"] = [ClimbingRate[idx] for idx in InBurst]\n",
    "    \n",
    "    # Shift the data one up to have the climbing rate between the current location and the next\n",
    "    data.ClimbingRate = data.ClimbingRate.shift(-1)\n",
    "    \n",
    "    # Calculate the running window/smoothed climbing rate\n",
    "    for BurstID in BurstIDs:\n",
    "        data.loc[data[\"BurstID\"]==BurstID,\"Smoothed_height-above-ellipsoid\"] = uniform_filter1d(data.loc[data[\"BurstID\"]==BurstID,\"height-above-ellipsoid\"], size=RunningWindowLength)\n",
    "    \n",
    "    data[\"SmoothedClimbingRate\"] = np.nan\n",
    "    \n",
    "    HeightDiff = data[\"Smoothed_height-above-ellipsoid\"].diff()\n",
    "    ClimbingRate = (HeightDiff/TimeDiff).tolist()\n",
    "    \n",
    "    # Enter ClimbingRate into the data, but only for the rows belonging to a burst\n",
    "    data.loc[data.index.isin(InBurst),\"SmoothedClimbingRate\"] = [ClimbingRate[idx] for idx in InBurst]\n",
    "    \n",
    "    # Shift the data one up to have the climbing rate between the current location and the next\n",
    "    data.SmoothedClimbingRate = data.SmoothedClimbingRate.shift(-1)\n",
    "    \n",
    "\n",
    "    for BurstID in BurstIDs:\n",
    "        \n",
    "        #-------------------#\n",
    "        #- Classify flight -#\n",
    "        #-------------------#\n",
    "            \n",
    "        # Set Flying to T when the ground speed is higher than MinGroundSpeed and to F if not\n",
    "        Flying = data[data[\"BurstID\"]==BurstID][\"ground-speed\"] >= MinGroundSpeed\n",
    "    \n",
    "        # Give each flight segment an ID (non-flight will also get an ID first)\n",
    "        FlyingID = (Flying == False).cumsum()\n",
    "\n",
    "        # Replace the IDs with na when Flying is False\n",
    "        FlyingID[(Flying == False)] = np.nan\n",
    "\n",
    "        # Check if there is another segment less than MinNonFlightTime away\n",
    "        if len(FlyingID.dropna().unique())>1:\n",
    "            for i in FlyingID.dropna().unique():\n",
    "                \n",
    "                if i <= min(FlyingID.dropna().unique()):\n",
    "                    idx = max(FlyingID[FlyingID==i].index)\n",
    "                    indices = [*range(idx+1,idx+MinNonFlightTime+1)]\n",
    "                    indices = [j for (j, v) in zip(indices, [item in FlyingID.index for item in indices]) if v]\n",
    "                    if len(FlyingID[indices].dropna())>0:\n",
    "                        idxs = FlyingID[indices].isnull()\n",
    "                        idxs = idxs[idxs].index\n",
    "                        FlyingID[idxs] = i\n",
    "\n",
    "                elif i >= max(FlyingID.dropna().unique()):\n",
    "                    idx = min(FlyingID[FlyingID==i].index)\n",
    "                    indices = [*range(idx-MinNonFlightTime,idx)]\n",
    "                    indices = [j for (j, v) in zip(indices, [item in FlyingID.index for item in indices]) if v]\n",
    "                    if len(FlyingID[indices].dropna())>0:\n",
    "                        idxs = FlyingID[indices].isnull()\n",
    "                        idxs = idxs[idxs].index\n",
    "                        FlyingID[idxs] = i\n",
    "\n",
    "                else:\n",
    "                    idx = max(FlyingID[FlyingID==i].index)\n",
    "                    indices = [*range(idx+1,idx+MinNonFlightTime+1)]\n",
    "                    indices = [j for (j, v) in zip(indices, [item in FlyingID.index for item in indices]) if v]\n",
    "                    if len(FlyingID[indices].dropna())>0:\n",
    "                        idxs = FlyingID[indices].isnull()\n",
    "                        idxs = idxs[idxs].index\n",
    "                        FlyingID[idxs] = i\n",
    "\n",
    "                    idx = min(FlyingID[FlyingID==i].index)\n",
    "                    indices = [*range(idx-MinNonFlightTime,idx)]\n",
    "                    indices = [j for (j, v) in zip(indices, [item in FlyingID.index for item in indices]) if v]\n",
    "                    if len(FlyingID[indices].dropna())>0:\n",
    "                        idxs = FlyingID[indices].isnull()\n",
    "                        idxs = idxs[idxs].index\n",
    "                        FlyingID[idxs] = i\n",
    "\n",
    "        # Give merged segments the same number\n",
    "        FlyingID2 = FlyingID.isnull().cumsum()\n",
    "\n",
    "        # Replace the values with nan where FlyingID is nan\n",
    "        FlyingID2[FlyingID.isnull()] = np.nan\n",
    "        FlyingID = FlyingID2\n",
    "\n",
    "        # Replace the IDs with na if the segment is shorter than MinFlightTime\n",
    "        for i in FlyingID.dropna().unique():\n",
    "            if len(FlyingID[FlyingID==i])<MinFlightTime:\n",
    "                FlyingID[FlyingID==i] = np.nan\n",
    "\n",
    "        # Enter the FlyingIDs in the data\n",
    "        data.loc[data[\"BurstID\"]==BurstID,\"FlyingID\"] = FlyingID\n",
    "\n",
    "        #---------------------#\n",
    "        #- Classify climbing -#\n",
    "        #---------------------#\n",
    "\n",
    "        \n",
    "        # Do this for every flying segment separately\n",
    "        for F_ID in FlyingID.dropna().unique():\n",
    "            \n",
    "            # Set Climbing to T when the climbing rate is higher than MinClimbingRate and to F if not\n",
    "            Climbing = data[(data[\"BurstID\"]==BurstID)&(data[\"FlyingID\"]==F_ID)][\"SmoothedClimbingRate\"] >= MinClimbingRate\n",
    "\n",
    "            # Give each flight segment an ID (non-flight will also get an ID first)\n",
    "            ClimbingID = (Climbing == False).cumsum()\n",
    "\n",
    "            # Replace the IDs with na when Flying is False\n",
    "            ClimbingID[(Climbing == False)] = np.nan\n",
    "\n",
    "            # Check if there is another segment less than MinNonFlightTime away\n",
    "            if len(ClimbingID.dropna().unique())>1:\n",
    "                for i in ClimbingID.dropna().unique():\n",
    "\n",
    "                    if i <= min(ClimbingID.dropna().unique()):\n",
    "                        idx = max(ClimbingID[ClimbingID==i].index)\n",
    "                        indices = [*range(idx+1,idx+MinNonFlightTime+1)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in ClimbingID.index for item in indices]) if v]\n",
    "                        if len(ClimbingID[indices].dropna())>0:\n",
    "                            idxs = ClimbingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            ClimbingID[idxs] = i\n",
    "                            \n",
    "                    elif i >= max(ClimbingID.dropna().unique()):\n",
    "                        idx = min(ClimbingID[ClimbingID==i].index)\n",
    "                        indices = [*range(idx-MinNonFlightTime,idx)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in ClimbingID.index for item in indices]) if v]\n",
    "                        if len(ClimbingID[indices].dropna())>0:\n",
    "                            idxs = ClimbingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            ClimbingID[idxs] = i\n",
    "                            \n",
    "                    else:\n",
    "                        \n",
    "                        idx = max(ClimbingID[ClimbingID==i].index)\n",
    "                        indices = [*range(idx+1,idx+MinNonFlightTime+1)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in ClimbingID.index for item in indices]) if v]\n",
    "                        if len(ClimbingID[indices].dropna())>0:\n",
    "                            idxs = ClimbingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            ClimbingID[idxs] = i\n",
    "                        \n",
    "                        idx = min(ClimbingID[ClimbingID==i].index)\n",
    "                        indices = [*range(idx-MinNonFlightTime,idx)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in ClimbingID.index for item in indices]) if v]\n",
    "                        if len(ClimbingID[indices].dropna())>0:\n",
    "                            idxs = ClimbingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            ClimbingID[idxs] = i\n",
    "\n",
    "            # Give merged segments the same number\n",
    "            ClimbingID2 = ClimbingID.isnull().cumsum()\n",
    "\n",
    "            # Replace the values with nan where FlyingID is nan\n",
    "            ClimbingID2[ClimbingID.isnull()] = np.nan\n",
    "            ClimbingID = ClimbingID2\n",
    "\n",
    "            # Replace the IDs with na if the segment is shorter than MinFlightTime\n",
    "            for i in ClimbingID.dropna().unique():\n",
    "                if len(ClimbingID[ClimbingID==i])<MinFlightTime:\n",
    "                    ClimbingID[ClimbingID==i] = np.nan\n",
    "\n",
    "            # Enter the ClimbingIDs in the data\n",
    "            data.loc[(data[\"BurstID\"]==BurstID)&(data[\"FlyingID\"]==F_ID),\"ClimbingID\"] = ClimbingID          \n",
    "\n",
    "        #--------------------#\n",
    "        #- Classify gliding -#\n",
    "        #--------------------#\n",
    "        \n",
    "        # Do this for every flying segment separately\n",
    "        for F_ID in FlyingID.dropna().unique():\n",
    "            \n",
    "            # Set Gliding to T when the climbing rate is higher than MinClimbingRate and to F if not\n",
    "            Gliding = data[(data[\"BurstID\"]==BurstID)&(data[\"FlyingID\"]==F_ID)][\"SmoothedClimbingRate\"] <= MaxDecliningRate\n",
    "\n",
    "            # Give each flight segment an ID (non-flight will also get an ID first)\n",
    "            GlidingID = (Gliding == False).cumsum()\n",
    "\n",
    "            # Replace the IDs with na when Flying is False\n",
    "            GlidingID[(Gliding == False)] = np.nan\n",
    "\n",
    "            # Check if there is another segment less than MinNonFlightTime away\n",
    "            if len(GlidingID.dropna().unique())>1:\n",
    "                for i in GlidingID.dropna().unique():\n",
    "                    \n",
    "                    if i <= min(GlidingID.dropna().unique()):\n",
    "                        idx = max(GlidingID[GlidingID==i].index)\n",
    "                        indices = [*range(idx+1,idx+MinNonFlightTime+1)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in GlidingID.index for item in indices]) if v]\n",
    "                        if len(GlidingID[indices].dropna())>0:\n",
    "                            idxs = GlidingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            GlidingID[idxs] = i\n",
    "                            \n",
    "                    elif i >= max(GlidingID.dropna().unique()):\n",
    "                        idx = min(GlidingID[GlidingID==i].index)\n",
    "                        indices = [*range(idx-MinNonFlightTime,idx)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in GlidingID.index for item in indices]) if v]\n",
    "                        if len(GlidingID[indices].dropna())>0:\n",
    "                            idxs = GlidingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            GlidingID[idxs] = i\n",
    "                            \n",
    "                    else:\n",
    "                        \n",
    "                        idx = max(GlidingID[GlidingID==i].index)\n",
    "                        indices = [*range(idx+1,idx+MinNonFlightTime+1)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in GlidingID.index for item in indices]) if v]\n",
    "                        if len(GlidingID[indices].dropna())>0:\n",
    "                            idxs = GlidingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            GlidingID[idxs] = i\n",
    "                        \n",
    "                        idx = min(GlidingID[GlidingID==i].index)\n",
    "                        indices = [*range(idx-MinNonFlightTime,idx)]\n",
    "                        indices = [j for (j, v) in zip(indices, [item in GlidingID.index for item in indices]) if v]\n",
    "                        if len(GlidingID[indices].dropna())>0:\n",
    "                            idxs = GlidingID[indices].isnull()\n",
    "                            idxs = idxs[idxs].index\n",
    "                            GlidingID[idxs] = i\n",
    "\n",
    "            # Give merged segments the same number\n",
    "            GlidingID2 = GlidingID.isnull().cumsum()\n",
    "\n",
    "            # Replace the values with nan where FlyingID is nan\n",
    "            GlidingID2[GlidingID.isnull()] = np.nan\n",
    "            GlidingID = GlidingID2\n",
    "\n",
    "            # Replace the IDs with na if the segment is shorter than MinFlightTime\n",
    "            for i in GlidingID.dropna().unique():\n",
    "                if len(GlidingID[GlidingID==i])<MinFlightTime:\n",
    "                    GlidingID[GlidingID==i] = np.nan\n",
    "\n",
    "            # Enter the GlidingIDs in the data\n",
    "            data.loc[(data[\"BurstID\"]==BurstID)&(data[\"FlyingID\"]==F_ID),\"GlidingID\"] = GlidingID\n",
    "            \n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FlightClassification()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FlightClassification()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_Q.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FlightClassification()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add elevation and altitude\n",
    "Elevation is separately downloaded from Movebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function\n",
    "def AddElevation():\n",
    "\n",
    "    #---------------------------#\n",
    "    #- Load the elevation data -#\n",
    "    #---------------------------#\n",
    "    \n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Elevation/\"+Pattern+\"*.csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_E = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data_E = pd.concat(data_E)\n",
    "    \n",
    "    # Only keep GPS data\n",
    "    data_E = data_E[data_E[\"sensor-type\"]==\"gps\"]\n",
    "\n",
    "    # Only keep locations that are not NA\n",
    "    data_E.dropna(subset=[\"location-long\",\"location-lat\"],inplace=True)\n",
    "\n",
    "    # Convert the timestamps\n",
    "    data_E[\"timestamp\"] = pd.to_datetime(data_E[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    # Sort the data\n",
    "    data_E.sort_values([\"tag-local-identifier\",\"timestamp\",\"gps:satellite-count\"], ascending=[True,True,True], inplace=True)\n",
    "\n",
    "    # Remove duplicated timestamps\n",
    "    data_E.drop_duplicates([\"timestamp\",\"tag-local-identifier\"],keep=\"first\",inplace=True)\n",
    "    \n",
    "    # Replace some column names\n",
    "    data_E.rename({\n",
    "        \"ASTER ASTGTM2 Quality Control\":\"QualityControlElevation\",\n",
    "        \"ASTER ASTGTM2 Elevation\":\"Elevation\"\n",
    "    }, axis=1, inplace=True)\n",
    "\n",
    "    # Only keep necessary columns\n",
    "    data_E = data_E[[\n",
    "        \"timestamp\",\n",
    "        \"tag-local-identifier\",\n",
    "        \"height-above-msl\",\n",
    "        \"QualityControlElevation\",\n",
    "        \"Elevation\"\n",
    "    ]]\n",
    "\n",
    "    #----------------------#\n",
    "    #- Load the main data -#\n",
    "    #----------------------#\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    #-----------------#\n",
    "    #- Add elevation -#\n",
    "    #-----------------#\n",
    "    \n",
    "    # Merge the data frames by timestamp and tag-local-identifier\n",
    "    data = data.merge(data_E,on=[\"timestamp\",\"tag-local-identifier\"],how=\"left\")   \n",
    "    \n",
    "    # Calculate altitude\n",
    "    data[\"Altitude\"] = data[\"height-above-msl\"] - data[\"Elevation\"]\n",
    "    \n",
    "    #-----------------#\n",
    "    #- Save the data -#\n",
    "    #-----------------#\n",
    "    \n",
    "    # Save the data into a new pkl file\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "Pattern = \"White Stork Affenberg\"\n",
    "file_name_in = \"DataAff_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddElevation()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCemter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "Pattern = \"*White Stork SW Germany Care Centre\"\n",
    "file_name_in = \"DataCC_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddElevation()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "Pattern = \"*White Stork SW Germany CASCB\"\n",
    "file_name_in = \"DataCASCB_TempWind_Q.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddElevation()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate wind components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def CalculateWind():\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    #--------------------------------------#\n",
    "    #- Calculate wind speed and direction -#\n",
    "    #--------------------------------------#\n",
    "    \n",
    "    # The U component of the wind is the west-east component\n",
    "    # The V component of the wind is the south-north component\n",
    "\n",
    "    # Calculate horizontal windspeed\n",
    "    data[\"WindSpeed_PL\"] = (data[\"U_Wind_PL\"]**2 + data[\"V_Wind_PL\"]**2)**0.5\n",
    "    \n",
    "    # Calculate wind direction\n",
    "    # Formula from https://www.eol.ucar.edu/content/wind-direction-quick-reference\n",
    "    # arctan2 calculates the direction from which the wind comes. +180 -> wind direction\n",
    "    data[\"WindDirection_PL\"] = np.arctan2(-data[\"U_Wind_PL\"],-data[\"V_Wind_PL\"])*(180/np.pi)+180\n",
    "    \n",
    "    # Make all angles smaller than 360\n",
    "    if len(data.loc[data[\"WindDirection_PL\"]>360])>0:\n",
    "        print(\"Some angles are larger than 360\")\n",
    "        data.loc[data[\"WindDirection_PL\"]>360] = data.loc[data[\"WindDirection_PL\"]>360] - 360 # Not tested, because this line is most likely not necessary\n",
    "        \n",
    "    \n",
    "    #---------------------------------------#\n",
    "    #- Calculate windsupport and crosswind -#\n",
    "    #---------------------------------------#\n",
    "\n",
    "    # Calculate the angle between the direction of the stork (heading) and the direction of the wind\n",
    "    data[\"Angle_heading_WindPL\"] = data[\"heading\"] - data[\"WindDirection_PL\"]\n",
    "\n",
    "    # Determine the a-angle to do the calculations with\n",
    "    data[\"Angle_a\"] = abs(data[\"Angle_heading_WindPL\"])\n",
    "\n",
    "    # Calculate the w-angle\n",
    "    data[\"Angle_w\"] = 90-data[\"Angle_a\"]\n",
    "\n",
    "    # Calculate windsupport\n",
    "    # Formula from https://doi.org/10.1186/2051-3933-1-4\n",
    "    # Windsupport is in the direction of the heading\n",
    "    data[\"WindSupport_PL\"] = data[\"WindSpeed_PL\"]*np.sin(data[\"Angle_w\"]*np.pi/180)\n",
    "    \n",
    "    # Calculate crosswind\n",
    "    # Formula from https://doi.org/10.1186/2051-3933-1-4\n",
    "    data[\"CrossWind_PL\"] = data[\"WindSpeed_PL\"]*np.cos(data[\"Angle_w\"]*np.pi/180)\n",
    "    \n",
    "    # Calculate Airspeed\n",
    "    # Formula from https://doi.org/10.1186/2051-3933-1-4\n",
    "    data[\"AirSpeed_PL\"] = ((data[\"ground-speed\"]-data[\"WindSupport_PL\"])**2 + data[\"CrossWind_PL\"]**2)**0.5\n",
    "    \n",
    "    #-----------------#\n",
    "    #- Save the data -#\n",
    "    #-----------------#\n",
    "    \n",
    "    # Save the data into a new pkl file\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateWind()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCemter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateWind()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateWind()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the start and end of daily migration flights\n",
    "\n",
    "- Start of flight is the start time of the first burst that ends at least 100 m from the first location of the day\n",
    "- Plus a lot of other refinements\n",
    "- End of flight is the end time (start + 10 minutes) of the last burst that increases the net displacement compared to the previous burst(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def FindMigFlight(MinDist=0.2,MinNoBursts=4,MinMigDist=50,SplitHours=1):\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Order the data frame by individual and timestamp\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\"], ascending=[True,True], inplace=True)\n",
    "    \n",
    "    data[\"UniqueNumber\"] = range(len(data))\n",
    "    \n",
    "    # Add new columns for the start and end of flight\n",
    "    data[\"StartFlight\"] = np.nan\n",
    "    data[\"EndFlight\"] = np.nan\n",
    "\n",
    "    for ind in data[\"tag-local-identifier\"].unique():\n",
    "        for Day in data[data[\"tag-local-identifier\"]==ind][\"Day\"].unique():\n",
    "            \n",
    "            # Check if the stork moved more than MinMigDist km on the given day\n",
    "            data_sub = data[(data[\"tag-local-identifier\"]==ind) & (data[\"Day\"]==Day)]\n",
    "            Distance_day = geopy.distance.distance((data_sub.iloc[0,data_sub.columns.get_loc(\"location-lat\")], data_sub.iloc[0,data_sub.columns.get_loc(\"location-long\")]),(data_sub.iloc[-1,data_sub.columns.get_loc(\"location-lat\")], data_sub.iloc[-1,data_sub.columns.get_loc(\"location-long\")])).km\n",
    "\n",
    "            # The daily distance should be at least MinMigDist\n",
    "            if Distance_day < MinMigDist:\n",
    "                continue\n",
    "                \n",
    "            # The last location should be south or west of the first location\n",
    "            if ~((data_sub[\"location-lat\"].iloc[0] > data_sub[\"location-lat\"].iloc[-1])|(data_sub[\"location-long\"].iloc[0] > data_sub[\"location-long\"].iloc[-1])):\n",
    "                continue\n",
    "\n",
    "            # Determine the end of the bursts for this specific day and individual\n",
    "            BurstStarts = data[(data[\"tag-local-identifier\"]==ind) & (data[\"Day\"]==Day) & (data[\"BelongsToBurst\"]==True)].groupby(\"BurstID\").first()\n",
    "            BurstEnds = data[(data[\"tag-local-identifier\"]==ind) & (data[\"Day\"]==Day) & (data[\"BelongsToBurst\"]==True)].groupby(\"BurstID\").last()\n",
    "\n",
    "            # Determine the first location of the day\n",
    "            DayStarts = data[(data[\"tag-local-identifier\"]==ind) & (data[\"Day\"]==Day)].iloc[0]\n",
    "\n",
    "            # Make a list to enter distances\n",
    "            k_Dist = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            k_BurstID = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            k_StartTime = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            k_BDist = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            k_LatF = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            k_LatL = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            k_LongF = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            k_LongL = [None] * len(BurstEnds[\"location-lat\"])\n",
    "            \n",
    "            for idx in range(len(BurstEnds)): \n",
    "                # Calculate the distance between the first location of the day and the last location of each burst\n",
    "                k_Dist[idx] = geopy.distance.distance((DayStarts[\"location-lat\"], DayStarts[\"location-long\"]),(BurstEnds[\"location-lat\"].iloc[idx], BurstEnds[\"location-long\"].iloc[idx])).km\n",
    "                k_BurstID[idx] = data[data.UniqueNumber==BurstEnds.UniqueNumber.iloc[idx]][\"BurstID\"].item()\n",
    "                k_StartTime[idx] = BurstEnds[\"Burst_A\"].iloc[idx]\n",
    "                k_BDist[idx] = geopy.distance.distance((BurstStarts[\"location-lat\"].iloc[idx], BurstStarts[\"location-long\"].iloc[idx]),(BurstEnds[\"location-lat\"].iloc[idx], BurstEnds[\"location-long\"].iloc[idx])).km\n",
    "                k_LatF[idx] = BurstStarts[\"location-lat\"].iloc[idx]\n",
    "                k_LatL[idx] = BurstEnds[\"location-lat\"].iloc[idx]\n",
    "                k_LongF[idx] = BurstStarts[\"location-long\"].iloc[idx]\n",
    "                k_LongL[idx] = BurstEnds[\"location-long\"].iloc[idx]\n",
    "\n",
    "            # Combine k into a data frame\n",
    "            k = pd.DataFrame({\"Dist\": k_Dist,\"BurstID\": k_BurstID,\"StartTime\": k_StartTime,\"BDist\": k_BDist,\"LatF\": k_LatF,\"LatL\": k_LatL,\"LongF\": k_LongF,\"LongL\": k_LongL})\n",
    "            \n",
    "            if not (len(k)>0):\n",
    "                continue\n",
    "                \n",
    "            # Find the locations where bursts are not consecutive\n",
    "            Breaks = list(k[k[\"StartTime\"].diff().dt.total_seconds()/60>25].index)\n",
    "            Breaks.insert(0,0)\n",
    "            Breaks.insert(len(Breaks),max(k.index))\n",
    "\n",
    "            BurstNo = [None] * len(k)\n",
    "            for i in range(len(Breaks)-1):\n",
    "                BurstNo[Breaks[i]:Breaks[i+1]] = [i]*len(BurstNo[Breaks[i]:Breaks[i+1]])\n",
    "                # The code above doesn't apply to the last item in the list (has to do with the way indexing works in Python-lists)\n",
    "                if Breaks[i+1] == (len(k)-1):\n",
    "                    BurstNo[Breaks[i]:(Breaks[i+1]+1)] = [i]*len(BurstNo[Breaks[i]:(Breaks[i+1]+1)])\n",
    "\n",
    "            k[\"BurstNo\"] = BurstNo\n",
    "\n",
    "            k[\"DistDiff\"] = k[\"Dist\"].diff()\n",
    "            \n",
    "            k = k[k[\"BDist\"]>MinDist]\n",
    "            \n",
    "            k2 = k.groupby(\"BurstNo\",as_index=False).agg(\n",
    "                    Number=(\"BurstNo\",'count'),\n",
    "                    FirstD=(\"Dist\",'first'),\n",
    "                    LastD=(\"Dist\",'last'),\n",
    "                    FirstT=(\"StartTime\",'first'),\n",
    "                    LastT=(\"StartTime\",'last'),\n",
    "                    BDistS=(\"BDist\",'sum'),\n",
    "                    DistDiffM=(\"DistDiff\",'mean'),\n",
    "                    FirstLat=(\"LatF\",'first'),\n",
    "                    LastLat=(\"LatL\",'last'),\n",
    "                    FirstLong=(\"LongF\",'first'),\n",
    "                    LastLong=(\"LongL\",'last')\n",
    "                )\n",
    "                \n",
    "            k2[\"CumDistDiff\"] = k2[\"FirstD\"].diff()#.shift(-1)\n",
    "            k2[\"SCDist\"] = k2[\"LastD\"] - k2[\"FirstD\"]\n",
    "            k2[\"LatDiff\"] = k2[\"FirstLat\"] - k2[\"LastLat\"]\n",
    "            k2[\"LongDiff\"] = k2[\"FirstLong\"] - k2[\"LastLong\"]\n",
    "            k2 = k2[(k2[\"LatDiff\"]>0)|(k2[\"LongDiff\"]>0)]\n",
    "                \n",
    "            if len(k2)>1:\n",
    "        \n",
    "                # Only keep \n",
    "                k2 = k2[(k2[\"DistDiffM\"]>0)|(k2[\"DistDiffM\"].isna())]\n",
    "\n",
    "                # Only keep consecutive burst parts when the cumulative daily distance in the parts increases and if there are a minimum of MinNoBursts consecutive bursts\n",
    "                k2 = k2[(k2[\"CumDistDiff\"]>0)|k2[\"CumDistDiff\"].isna()|(k2[\"SCDist\"]>(k2[\"Number\"]*MinDist))]\n",
    "\n",
    "                k2[\"CumDistDiff2\"] = k2[\"LastD\"] - k2[\"FirstD\"]\n",
    "\n",
    "                k2 = k2[(k2[\"Number\"]<2)|(k2[\"SCDist\"]>0)]\n",
    "                    \n",
    "            k = k[k.BurstNo.isin(k2.BurstNo)]\n",
    "            \n",
    "                                \n",
    "            if len(k2)>1:\n",
    "                k[\"DiffTime\"] = (k[\"StartTime\"].diff().dt.total_seconds()/3600).shift(-1)\n",
    "                if max(k[\"DiffTime\"])>=SplitHours:\n",
    "\n",
    "                    BNos = [*(k[k[\"DiffTime\"]>=SplitHours][\"BurstNo\"])]\n",
    "                    BNos = [[*k.BurstNo.unique()].index(BN) for BN in BNos] + [[*k.BurstNo.unique()].index(BN)+1 for BN in BNos]\n",
    "                    BNos = k.BurstNo.unique()[BNos]\n",
    "                    k3 = k2[k2[\"BurstNo\"].isin(BNos)]\n",
    "\n",
    "                    if len(k3)>1:\n",
    "                        k3 = k3[~(k3[\"Number\"]==max(k3[\"Number\"]))]\n",
    "                        k3 = k3[k3[\"SCDist\"]<(MinMigDist/4)]\n",
    "                    k = k[~k[\"BurstNo\"].isin(k3[\"BurstNo\"])]\n",
    "                    k[\"DiffTime\"] = (k[\"StartTime\"].diff().dt.total_seconds()/3600).shift(-1)\n",
    "                    \n",
    "                if max(k[\"DiffTime\"])>=SplitHours:\n",
    "\n",
    "                    BNos = [*(k[k[\"DiffTime\"]>=SplitHours][\"BurstNo\"])]\n",
    "                    BNos = [[*k.BurstNo.unique()].index(BN) for BN in BNos] + [[*k.BurstNo.unique()].index(BN)+1 for BN in BNos]\n",
    "                    BNos = k.BurstNo.unique()[BNos]\n",
    "                    k3 = k2[k2[\"BurstNo\"].isin(BNos)]\n",
    "                    \n",
    "                    if len(k3)>1:\n",
    "                        k3 = k3[k3[\"SCDist\"]<(MinMigDist/4)]\n",
    "                    k = k[~k[\"BurstNo\"].isin(k3[\"BurstNo\"])]\n",
    "                    \n",
    "            if len(k)>0:\n",
    "                # The start of the daily migration is the start of the first burst in k and the end is the start of the last burst + 10 minutes\n",
    "                data.loc[(data[\"tag-local-identifier\"]==ind) & (data[\"Day\"]==Day),\"StartFlight\"] = k[\"StartTime\"].iloc[0]\n",
    "                data.loc[(data[\"tag-local-identifier\"]==ind) & (data[\"Day\"]==Day),\"EndFlight\"] = k[\"StartTime\"].iloc[-1] + datetime.timedelta(minutes = 10)\n",
    "                \n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindMigFlight()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindMigFlight()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_R.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindMigFlight()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Subset the data\n",
    "- Keep data during flight\n",
    "- Keep data during migration days\n",
    "- Keep data within the segment\n",
    "- Keep individuals that cover the entire segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def SubsetData():\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Keep only data during flight (the main migration flight)\n",
    "    data = data[(~data.StartFlight.isna())&(~data.EndFlight.isna())]\n",
    "    data = data[data.timestamp>=data.StartFlight]\n",
    "    data = data[data.timestamp<(data.EndFlight+datetime.timedelta(minutes=5))]\n",
    "\n",
    "    # Keep only migration days (>50 km)\n",
    "    Subset = data.groupby([\"Day\",\"tag-local-identifier\"],as_index=False)[[\"location-long\",\"location-lat\"]].agg(['first','last'])\n",
    "    Subset.reset_index(level=0, inplace=True)\n",
    "    Subset.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    Dist = [None] * len(Subset)\n",
    "    for i in range(len(Subset)):\n",
    "        Dist[i] = geopy.distance.distance((Subset[\"location-lat\"][\"first\"][i],Subset[\"location-long\"][\"first\"][i]),(Subset[\"location-lat\"][\"last\"][i],Subset[\"location-long\"][\"last\"][i])).km\n",
    "\n",
    "    Subset[\"Dist\"] = Dist\n",
    "    Subset = Subset[Subset[\"Dist\"]>=50]\n",
    "\n",
    "    data[\"MigDay\"] = False\n",
    "    for i in data[\"tag-local-identifier\"].unique():\n",
    "        data.loc[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"].isin(Subset[Subset[\"tag-local-identifier\"]==i][\"Day\"])),[\"MigDay\"]] = True\n",
    "        \n",
    "    data = data[data[\"MigDay\"]]\n",
    "\n",
    "    # Keep only data during migration / within the segment\n",
    "    \n",
    "    # Find first location south of the segment\n",
    "    Subset = data.groupby([\"Burst_A\",\"tag-local-identifier\"],as_index=False)[\"location-lat\"].first()\n",
    "    Subset = Subset[Subset[\"location-lat\"]<44.0]\n",
    "    \n",
    "    # Remove individuals that did not reach south of 44.0\n",
    "    data = data[data[\"tag-local-identifier\"].isin(Subset[\"tag-local-identifier\"])]\n",
    "    \n",
    "    # Remove the data from the first Burst_A the individual is south of the segment\n",
    "    Subset = Subset.groupby([\"tag-local-identifier\"],as_index=False).first()\n",
    "    data = data[data[\"Burst_A\"]<data[\"tag-local-identifier\"].map(Subset.set_index(\"tag-local-identifier\")[\"Burst_A\"].to_dict())]\n",
    "    \n",
    "    # Find the first Burst_A that ends in the segment\n",
    "    Subset = data.groupby([\"Burst_A\",\"tag-local-identifier\"],as_index=False)[\"location-lat\"].last()\n",
    "    Subset = Subset[Subset[\"location-lat\"]<47.5]\n",
    "    Subset = Subset.groupby([\"tag-local-identifier\"],as_index=False).first()\n",
    "    \n",
    "    # Remove data before the first Burst_A that ends within the segment\n",
    "    data = data[~(data[\"Burst_A\"]<data[\"tag-local-identifier\"].map(Subset.set_index(\"tag-local-identifier\")[\"Burst_A\"].to_dict()))]\n",
    "    \n",
    "    # Add an individual ID\n",
    "    data[\"Individual\"] = data[\"Aviary\"].map(str)+\"_\"+data[\"tag-local-identifier\"].map(str)\n",
    "    \n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_S.pkl\"\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SubsetData()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_S.pkl\"\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SubsetData()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_S.pkl\"\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SubsetData()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Subset data for improvement and pre-segment flight\n",
    "- Keep data during flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def SubsetData2():\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Add an individual ID\n",
    "    data[\"Individual\"] = data[\"Aviary\"].map(str)+\"_\"+data[\"tag-local-identifier\"].map(str)\n",
    "\n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_out = \"DataAff_TempWind_S2.pkl\"\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SubsetData2()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCC_TempWind_S2.pkl\"\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SubsetData2()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCASCB_TempWind_S2.pkl\"\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "SubsetData2()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine migration timing and minimum latitude for first and second year\n",
    "- The moment a stork enters the segment is used for migration timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "def SaveAff():\n",
    "    \n",
    "    # Load data and convert timestamps\n",
    "    data = pd.read_csv(data_folder+\"SecondMigration/White Stork Affenberg releases MPIAB.csv\",sep=',',low_memory=False)\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    # Load the release-death data\n",
    "    ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)\n",
    "    \n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    \n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "    \n",
    "    # Add the start-year of an individual to the data\n",
    "    Start_year = data.groupby(\"tag-local-identifier\")[\"timestamp\"].min().dt.year.reset_index()\n",
    "    data[\"StartYear\"] = data[\"tag-local-identifier\"].map(Start_year.set_index(\"tag-local-identifier\")[\"timestamp\"].to_dict())\n",
    "    \n",
    "    # Save data as .pkl file\n",
    "    data.to_pickle(data_folder+\"Second_DataAff_All_Second.pkl\")\n",
    "    \n",
    "start = datetime.datetime.now()\n",
    "aviary = [\"Affenberg_2019\",\"Affenberg_2020\"]\n",
    "SaveAff()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "def SaveCC():\n",
    "    \n",
    "    # Load data and convert timestamps\n",
    "    data = pd.read_csv(data_folder+\"SecondMigration/LifeTrack White Stork SW Germany Care Centre Releases.csv\",sep=',',low_memory=False)\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    # Load the release-death data\n",
    "    ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)\n",
    "    \n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    \n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "    \n",
    "    # Add the start-year of an individual to the data\n",
    "    Start_year = data.groupby(\"tag-local-identifier\")[\"timestamp\"].min().dt.year.reset_index()\n",
    "    data[\"StartYear\"] = data[\"tag-local-identifier\"].map(Start_year.set_index(\"tag-local-identifier\")[\"timestamp\"].to_dict())\n",
    "    \n",
    "    # Save data as .pkl file\n",
    "    data.to_pickle(data_folder+\"Second_DataCC_All_Second.pkl\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "aviary = [\"CareCenter_2019\",\"CareCenter_2020\"]\n",
    "SaveCC()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "def SaveCASCB():\n",
    "    \n",
    "    # Load data and convert timestamps\n",
    "    data = pd.read_csv(data_folder+\"SecondMigration/LifeTrack White Stork SW Germany CASCB.csv\",sep=',',low_memory=False)\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    # Load the release-death data\n",
    "    ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)\n",
    "    \n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    \n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "    \n",
    "    # Add the start-year of an individual to the data\n",
    "    Start_year = data.groupby(\"tag-local-identifier\")[\"timestamp\"].min().dt.year.reset_index()\n",
    "    data[\"StartYear\"] = data[\"tag-local-identifier\"].map(Start_year.set_index(\"tag-local-identifier\")[\"timestamp\"].to_dict())\n",
    "    \n",
    "    # Save data as .pkl file\n",
    "    data.to_pickle(data_folder+\"Second_DataCASCB_All_Second.pkl\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "aviary = [\"CASCB_East\",\"CASCB_West\"]\n",
    "SaveCASCB()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def FindMigTL():\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Order the data frame by individual and timestamp\n",
    "    data.sort_values([\"individual-local-identifier\",\"timestamp\"], ascending=[True,True], inplace=True)\n",
    "\n",
    "    # Convert the timestamps\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    # Add the start-year of the data\n",
    "    Start_year = data.groupby(\"individual-local-identifier\")[\"timestamp\"].min().dt.year.reset_index()\n",
    "    data[\"StartYear\"] = data[\"individual-local-identifier\"].map(Start_year.set_index(\"individual-local-identifier\")[\"timestamp\"].to_dict())\n",
    "    \n",
    "    # Only keep storks that started in 2019 or 2020\n",
    "    data = data[(data[\"StartYear\"]==2019)|(data[\"StartYear\"]==2020)]\n",
    "\n",
    "    # Add the date (without time) to the data frame\n",
    "    data[\"Day\"] = data[\"timestamp\"].dt.date\n",
    "    \n",
    "    # Load the release-death data\n",
    "    ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)\n",
    "\n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "    \n",
    "    # Sort the data\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\",\"gps:satellite-count\"], ascending=[True,True,True], inplace=True)\n",
    "\n",
    "    # Remove duplicated timestamps\n",
    "    data.drop_duplicates([\"timestamp\",\"tag-local-identifier\"],keep=\"first\",inplace=True)\n",
    "\n",
    "    # Only keep GPS data\n",
    "    data = data[data[\"sensor-type\"]==\"gps\"]\n",
    "\n",
    "    # Only keep locations that are not NA\n",
    "    data.dropna(subset=[\"location-long\",\"location-lat\"],inplace=True)\n",
    "\n",
    "    # Make some empty lists to enter the data into\n",
    "    \n",
    "    # First year\n",
    "    # Tag number\n",
    "    SEM_tag1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # First date for the tag/individual\n",
    "    SEM_FirstDate1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_FirstLat1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_FirstLong1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # First day in segment\n",
    "    SEM_FirstSegm1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_FirstSegmLat1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # Minimum and maximum latitude\n",
    "    SEM_MinLat1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MinLong1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MinLatDate1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MaxLat1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MaxLatDate1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # Date of death\n",
    "    SEM_Death1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # Date of release\n",
    "    SEM_Release1 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # Second year\n",
    "    # Tag number\n",
    "    SEM_tag2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # First date for the tag/individual\n",
    "    SEM_FirstDate2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_FirstLat2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_FirstLong2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # First day in segment\n",
    "    SEM_FirstSegm2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_FirstSegmLat2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # Minimum and maximum latitude\n",
    "    SEM_MinLat2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MinLong2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MinLatDate2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MaxLat2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    SEM_MaxLatDate2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # Date of death\n",
    "    SEM_Death2 = [None] * len(data[\"tag-local-identifier\"].unique())\n",
    "    \n",
    "    # Longitude\n",
    "    Long__ = None\n",
    "\n",
    "    #--------------#\n",
    "    #- First year -#\n",
    "    #--------------#\n",
    "    \n",
    "    # Remove data from 1 July in the year after tagging\n",
    "    data2 = data[data[\"timestamp\"]<[datetime.datetime(y+1,7,1) for y in data[\"StartYear\"]]]\n",
    "    \n",
    "    # Remove data before 1 July in the year of tagging\n",
    "    data2 = data2[data2[\"timestamp\"]>=[datetime.datetime(y,7,1) for y in data2[\"StartYear\"]]]\n",
    "\n",
    "    for idx,ind in enumerate(data[\"tag-local-identifier\"].unique()):\n",
    "        \n",
    "        Long__ = None\n",
    "        StartYear = None\n",
    "        N_MinLat = None\n",
    "        N_MinLatD = None\n",
    "        \n",
    "        # Subset the data so that it only contains individual 'ind'\n",
    "        data_sub = data2[data2[\"tag-local-identifier\"]==ind]\n",
    "        \n",
    "        # Insert the tag number in the list\n",
    "        SEM_tag1[idx] = ind\n",
    "        \n",
    "        if len(data_sub)<1:\n",
    "            continue\n",
    "        \n",
    "        StartYear = data_sub[\"StartYear\"].iloc[0]\n",
    "        \n",
    "        # Insert the date of death\n",
    "        SEM_Death1[idx] = data_sub[\"Death\"].iloc[0]\n",
    "        \n",
    "        # Insert the date of release\n",
    "        SEM_Release1[idx] = data_sub[\"Release\"].iloc[0]\n",
    "        \n",
    "        # Insert the first date for the individual\n",
    "        SEM_FirstDate1[idx] = data_sub[\"Day\"].min()\n",
    "        SEM_FirstLat1[idx] = data_sub[data_sub[\"Day\"]==data_sub[\"Day\"].min()][\"location-lat\"].iloc[0]\n",
    "        SEM_FirstLong1[idx] = data_sub[data_sub[\"Day\"]==data_sub[\"Day\"].min()][\"location-long\"].iloc[0]\n",
    "        \n",
    "        # Determine the maximum (most northern) latitude before 1 January (to exclude northward migration in the next year)\n",
    "        data_sub2 = data_sub[data_sub[\"timestamp\"]<=[datetime.datetime(y,12,31) for y in data_sub[\"StartYear\"]]]\n",
    "        \n",
    "        if len(data_sub2)>0:\n",
    "            SEM_MaxLat1[idx] = data_sub2[\"location-lat\"].max()\n",
    "            SEM_MaxLatDate1[idx] = data_sub2[data_sub2[\"location-lat\"]==data_sub2[\"location-lat\"].max()][\"Day\"].iloc()[0]\n",
    "        \n",
    "                        \n",
    "        # Find the last day north of the segment before 1 November\n",
    "        data_sub2 = data_sub[data_sub[\"timestamp\"]<=[datetime.datetime(y,11,1) for y in data_sub[\"StartYear\"]]]\n",
    "        if len(data_sub2[data_sub2[\"location-lat\"]>=47.5][\"Day\"])>1:\n",
    "            SEM_FirstSegm1[idx] = data_sub2[data_sub2[\"location-lat\"]>=47.5][\"Day\"].iloc()[-1]\n",
    "            SEM_FirstSegmLat1[idx] = data_sub2[data_sub2[\"location-lat\"]>=47.5][\"location-lat\"].iloc()[-1]\n",
    "            Long__ = data_sub2[data_sub2[\"location-lat\"]>=47.5][\"location-long\"].iloc()[-1]\n",
    "        \n",
    "        # If the longitude is >9, set to None\n",
    "        if not Long__ is None:\n",
    "            if Long__>9:\n",
    "                SEM_FirstSegm1[idx] = None\n",
    "                SEM_FirstSegmLat1[idx] = None\n",
    "                    \n",
    "        # Determine the minimum latitude\n",
    "        SEM_MinLat1[idx] = data_sub[\"location-lat\"].min()\n",
    "        SEM_MinLatDate1[idx] = data_sub[data_sub[\"location-lat\"]==data_sub[\"location-lat\"].min()][\"Day\"].iloc()[0]\n",
    "        SEM_MinLong1[idx] = data_sub[data_sub[\"location-lat\"]==data_sub[\"location-lat\"].min()][\"location-long\"].iloc()[0]\n",
    "        \n",
    "        N_MinLat = SEM_MinLat1[idx]+0.1\n",
    "        if len(data_sub[data_sub[\"location-lat\"]<N_MinLat][\"Day\"])>0:\n",
    "            N_MinLatD = data_sub[data_sub[\"location-lat\"]<N_MinLat][\"Day\"].iloc()[0]\n",
    "        \n",
    "        # Set to NA when MinLatDate or SEM_MinLatDate1 is less than 3 days from death\n",
    "        if  not pd.isnull(SEM_Death1[idx]):\n",
    "            if (SEM_FirstSegm1[idx] is not None):\n",
    "                if SEM_Death1[idx]<(SEM_FirstSegm1[idx]+datetime.timedelta(days=3)):\n",
    "                    SEM_FirstSegm1[idx] = None\n",
    "                    SEM_FirstSegmLat1[idx] = None\n",
    "   \n",
    "            if (N_MinLatD is not None):\n",
    "                if SEM_Death1[idx]<(N_MinLatD+datetime.timedelta(days=3)):\n",
    "                    SEM_MinLatDate1[idx] = None\n",
    "                    SEM_MinLat1[idx] = None\n",
    "                    SEM_MinLong1[idx] = None\n",
    "                    \n",
    "            if (SEM_Release1[idx] is not None):\n",
    "                if SEM_Death1[idx]<(SEM_Release1[idx]+datetime.timedelta(days=5)):\n",
    "                    SEM_MinLatDate1[idx] = None\n",
    "                    SEM_MinLat1[idx] = None\n",
    "                    SEM_MinLong1[idx] = None\n",
    "                    \n",
    "        # Set to None when MinLatDate and MaxLatDate are less than 3 days apart\n",
    "        if (SEM_MinLatDate1[idx] is not None):\n",
    "            if (SEM_MaxLatDate1[idx] is not None):\n",
    "                if SEM_MinLatDate1[idx]<(SEM_MaxLatDate1[idx]+datetime.timedelta(days=3)):\n",
    "                    if SEM_MaxLatDate1[idx]<(SEM_MinLatDate1[idx]+datetime.timedelta(days=3)):\n",
    "                        SEM_MinLatDate1[idx] = None\n",
    "                        SEM_MinLat1[idx] = None\n",
    "                        SEM_MinLong1[idx] = None\n",
    "                        \n",
    "        # If the MinLatDate is before 1 August, set to None\n",
    "        if (SEM_MinLatDate1[idx] is not None):\n",
    "            if SEM_MinLatDate1[idx]<datetime.date(StartYear,8,15):\n",
    "                SEM_MinLatDate1[idx] = None\n",
    "                SEM_MinLat1[idx] = None\n",
    "                SEM_MinLong1[idx] = None\n",
    "\n",
    "    \n",
    "    #---------------#\n",
    "    #- Second year -#\n",
    "    #---------------#\n",
    "    \n",
    "    # Remove data from 1 July in the second year after tagging\n",
    "    data2 = data[data[\"timestamp\"]<[datetime.datetime(y+2,7,1) for y in data[\"StartYear\"]]]\n",
    "    \n",
    "    # Remove data before 1 July in the year after tagging\n",
    "    data2 = data2[data2[\"timestamp\"]>=[datetime.datetime(y+1,7,1) for y in data2[\"StartYear\"]]]\n",
    "\n",
    "    for idx,ind in enumerate(data[\"tag-local-identifier\"].unique()):\n",
    "        \n",
    "        Long__ = None\n",
    "        StartYear = None\n",
    "        N_MinLat = None\n",
    "        N_MinLatD = None\n",
    "        \n",
    "        # Subset the data so that it only contains individual 'ind'\n",
    "        data_sub = data2[data2[\"tag-local-identifier\"]==ind]\n",
    "        \n",
    "        # Insert the tag number in the list\n",
    "        SEM_tag2[idx] = ind\n",
    "\n",
    "        if len(data_sub)<1:\n",
    "            continue\n",
    "\n",
    "        StartYear = data_sub[\"StartYear\"].iloc[0]\n",
    "        \n",
    "        # Insert the date of death\n",
    "        SEM_Death2[idx] = data_sub[\"Death\"].iloc[0]\n",
    "        \n",
    "        # Insert the first date for the individual\n",
    "        SEM_FirstDate2[idx] = data_sub[\"Day\"].min()\n",
    "        SEM_FirstLat2[idx] = data_sub[data_sub[\"Day\"]==data_sub[\"Day\"].min()][\"location-lat\"].iloc[0]\n",
    "        SEM_FirstLong2[idx] = data_sub[data_sub[\"Day\"]==data_sub[\"Day\"].min()][\"location-long\"].iloc[0]\n",
    "\n",
    "        # Determine the maximum (most northern) latitude before 1 January (to exclude northward migration in the next year)\n",
    "        data_sub2 = data_sub[data_sub[\"timestamp\"]<=[datetime.datetime(y+1,12,31) for y in data_sub[\"StartYear\"]]]\n",
    "\n",
    "        if len(data_sub2)>0:\n",
    "            SEM_MaxLat2[idx] = data_sub2[\"location-lat\"].max()\n",
    "            SEM_MaxLatDate2[idx] = data_sub2[data_sub2[\"location-lat\"]==data_sub2[\"location-lat\"].max()][\"Day\"].iloc()[0]\n",
    "\n",
    "        # Find the last day north of the segment before 1 November\n",
    "        data_sub2 = data_sub[data_sub[\"timestamp\"]<=[datetime.datetime(y+1,11,1) for y in data_sub[\"StartYear\"]]]\n",
    "        if len(data_sub2[data_sub2[\"location-lat\"]>=47.5][\"Day\"])>1:\n",
    "            SEM_FirstSegm2[idx] = data_sub2[data_sub2[\"location-lat\"]>=47.5][\"Day\"].iloc()[-1]\n",
    "            SEM_FirstSegmLat2[idx] = data_sub2[data_sub2[\"location-lat\"]>=47.5][\"location-lat\"].iloc()[-1]\n",
    "            Long__ = data_sub2[data_sub2[\"location-lat\"]>=47.5][\"location-long\"].iloc()[-1]\n",
    "        \n",
    "        # If the longitude is >9, set to None\n",
    "        if not Long__ is None:\n",
    "            if Long__>9:\n",
    "                SEM_FirstSegm2[idx] = None\n",
    "                SEM_FirstSegmLat2[idx] = None\n",
    "\n",
    "        # Determine the minimum latitude\n",
    "        SEM_MinLat2[idx] = data_sub[\"location-lat\"].min()\n",
    "        SEM_MinLatDate2[idx] = data_sub[data_sub[\"location-lat\"]==data_sub[\"location-lat\"].min()][\"Day\"].iloc()[0]\n",
    "        SEM_MinLong2[idx] = data_sub[data_sub[\"location-lat\"]==data_sub[\"location-lat\"].min()][\"location-long\"].iloc()[0]\n",
    "        \n",
    "        N_MinLat = SEM_MinLat1[idx]+0.1\n",
    "        if len(data_sub[data_sub[\"location-lat\"]<N_MinLat][\"Day\"])>0:\n",
    "            N_MinLatD = data_sub[data_sub[\"location-lat\"]<N_MinLat][\"Day\"].iloc()[0]\n",
    "        \n",
    "        # Set to NA when MinLatDate or SEM_MinLatDate1 is less than 3 days from death\n",
    "        if  not pd.isnull(SEM_Death2[idx]):\n",
    "            if (SEM_FirstSegm2[idx] is not None):\n",
    "                if SEM_Death2[idx]<(SEM_FirstSegm2[idx]+datetime.timedelta(days=3)):\n",
    "                    SEM_FirstSegm2[idx] = None\n",
    "                    SEM_FirstSegmLat2[idx] = None\n",
    "   \n",
    "            if (N_MinLatD is not None):\n",
    "                if SEM_Death2[idx]<(N_MinLatD+datetime.timedelta(days=3)):\n",
    "                    SEM_MinLatDate2[idx] = None\n",
    "                    SEM_MinLat2[idx] = None\n",
    "                    SEM_MinLong2[idx] = None\n",
    "                    \n",
    "\n",
    "        # Set to None when MinLatDate and MaxLatDate are less than 3 days apart\n",
    "        if (SEM_MinLatDate2[idx] is not None):\n",
    "            if (SEM_MaxLatDate2[idx] is not None):\n",
    "                if SEM_MinLatDate2[idx]<(SEM_MaxLatDate2[idx]+datetime.timedelta(days=3)):\n",
    "                    if SEM_MaxLatDate2[idx]<(SEM_MinLatDate2[idx]+datetime.timedelta(days=3)):\n",
    "                        SEM_MinLatDate2[idx] = None\n",
    "                        SEM_MinLat2[idx] = None\n",
    "                        SEM_MinLong2[idx] = None\n",
    "                        \n",
    "        # If the MinLatDate is before 1 August, set to None\n",
    "        if (SEM_MinLatDate2[idx] is not None):\n",
    "            if SEM_MinLatDate2[idx]<datetime.date(StartYear,8,15):\n",
    "                SEM_MinLatDate2[idx] = None\n",
    "                SEM_MinLat2[idx] = None\n",
    "                SEM_MinLong2[idx] = None\n",
    "        \n",
    "\n",
    "            \n",
    "    # Bind the data together into a data frame\n",
    "    SegTim_MinLat = pd.DataFrame({\n",
    "        \"tag-local-identifier\": SEM_tag1,\n",
    "        \"FirstDate1\": SEM_FirstDate1,\n",
    "        \"FirstLat1\": SEM_FirstLat1,\n",
    "        \"FirstLong1\": SEM_FirstLong1,\n",
    "        \"FirstSegm1\": SEM_FirstSegm1,\n",
    "        \"FirstSegmLat1\": SEM_FirstSegmLat1,\n",
    "        \"MinLat1\": SEM_MinLat1,\n",
    "        \"MinLong1\": SEM_MinLong1,\n",
    "        \"MinLatDate1\": SEM_MinLatDate1,\n",
    "        \"MaxLat1\": SEM_MaxLat1,\n",
    "        \"MaxLatDate1\": SEM_MaxLatDate1,\n",
    "        \"Death\": SEM_Death1,\n",
    "        \"tag-local-identifier2\": SEM_tag2,\n",
    "        \"FirstDate2\": SEM_FirstDate2,\n",
    "        \"FirstLat2\": SEM_FirstLat1,\n",
    "        \"FirstLong2\": SEM_FirstLong1,\n",
    "        \"FirstSegm2\": SEM_FirstSegm2,\n",
    "        \"FirstSegmLat2\": SEM_FirstSegmLat2,\n",
    "        \"MinLat2\": SEM_MinLat2,\n",
    "        \"MinLong2\": SEM_MinLong2,\n",
    "        \"MinLatDate2\": SEM_MinLatDate2,\n",
    "        \"MaxLat2\": SEM_MaxLat2,\n",
    "        \"MaxLatDate2\": SEM_MaxLatDate2,\n",
    "        \"Death2\": SEM_Death2\n",
    "    })\n",
    "    \n",
    "    # Add aviary\n",
    "    SegTim_MinLat[\"Aviary\"] = Aviary\n",
    "    \n",
    "    # Add Individual\n",
    "    SegTim_MinLat[\"Individual\"] = SegTim_MinLat[\"Aviary\"].map(str)+\"_\"+SegTim_MinLat[\"tag-local-identifier\"].map(str)\n",
    "\n",
    "    # Calculate the distance between the first location and the most southern latitude location\n",
    "    Distances1 = [None] * len(SegTim_MinLat)\n",
    "    Distances2 = [None] * len(SegTim_MinLat)\n",
    "    for row in range(len(SegTim_MinLat)):\n",
    "        if (not np.isnan(SegTim_MinLat[\"MinLat1\"].iloc()[row])):\n",
    "            if(not np.isnan(SegTim_MinLat[\"FirstLat1\"].iloc()[row])):\n",
    "                Distances1[row] = geopy.distance.distance((SegTim_MinLat[\"FirstLat1\"].iloc()[row], SegTim_MinLat[\"FirstLong1\"].iloc()[row]),(SegTim_MinLat[\"MinLat1\"].iloc()[row], SegTim_MinLat[\"MinLong1\"].iloc()[row])).km\n",
    "        if (not np.isnan(SegTim_MinLat[\"MinLat2\"].iloc()[row])):\n",
    "            if(not np.isnan(SegTim_MinLat[\"FirstLat2\"].iloc()[row])):\n",
    "                Distances2[row] = geopy.distance.distance((SegTim_MinLat[\"FirstLat2\"].iloc()[row], SegTim_MinLat[\"FirstLong2\"].iloc()[row]),(SegTim_MinLat[\"MinLat2\"].iloc()[row], SegTim_MinLat[\"MinLong2\"].iloc()[row])).km\n",
    "    SegTim_MinLat[\"Distance1\"] = Distances1\n",
    "    SegTim_MinLat[\"Distance2\"] = Distances2\n",
    "    \n",
    "\n",
    "    # Save the data\n",
    "    SegTim_MinLat.to_csv(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objects\n",
    "aviary = [\"Affenberg_2019\",\"Affenberg_2020\"]\n",
    "Aviary = \"Affenberg\"\n",
    "file_name_in = \"Second_DataAff_All_Second.pkl\"\n",
    "file_name_out = \"DataAff_SegTimMinLat.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindMigTL()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objects\n",
    "aviary = [\"CareCenter_2019\",\"CareCenter_2020\"]\n",
    "Aviary = \"CareCenter\"\n",
    "file_name_in = \"Second_DataCC_All_Second.pkl\"\n",
    "file_name_out = \"DataCC_SegTimMinLat.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindMigTL()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objects\n",
    "aviary = [\"CASCB_East\",\"CASCB_West\"]\n",
    "Aviary = \"CASCB\"\n",
    "file_name_in = \"Second_DataCASCB_All_Second.pkl\"\n",
    "file_name_out = \"DataCASCB_SegTimMinLat.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FindMigTL()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"SegTimMinLat\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ACC data for calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of columns to keep\n",
    "columns_to_keep = [\n",
    "    \"timestamp\",\n",
    "    \"tag-local-identifier\",\n",
    "    \"individual-local-identifier\",\n",
    "    \"eobs:accelerations-raw\",\n",
    "    \"sensor-type\"\n",
    "]\n",
    "\n",
    "# Load the release-death data\n",
    "ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def function_prep_acc(Released):\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_folder+file_name_in,sep=\",\",low_memory=False)\n",
    "\n",
    "    # Only keep columns that are necessary for further steps in the analyses\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Only keep ACC data\n",
    "    data = data[data[\"sensor-type\"]==\"acceleration\"]\n",
    "\n",
    "    # Convert the timestamps\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    # Remove data before release and after death\n",
    "    \n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "   \n",
    "    # Sort the data\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\"], ascending=[True,True], inplace=True)\n",
    "\n",
    "    #--- Split the ACC into colums X Y Z ---#\n",
    "    \n",
    "    # Make empty list to store the values in\n",
    "    timestamp = []\n",
    "    identifier = []\n",
    "    X_raw = []\n",
    "    Y_raw = []\n",
    "    Z_raw = []\n",
    "    ACC_num = []\n",
    "    \n",
    "    for ind in data[\"tag-local-identifier\"].unique():\n",
    "        timestamp__ = []\n",
    "        identifier__ = []\n",
    "        X_raw__ = []\n",
    "        Y_raw__ = []\n",
    "        Z_raw__ = []\n",
    "        ACC_num__ = []\n",
    "\n",
    "        data2 = data[data[\"tag-local-identifier\"]==ind]\n",
    "        \n",
    "        # Split the ACC values into columns\n",
    "        for i in range(len(data2)): \n",
    "\n",
    "            # Split the ACC values\n",
    "            SplittedACC = data2.iloc[i][\"eobs:accelerations-raw\"].split()\n",
    "            timestamp_ = np.repeat(data2.iloc[i][\"timestamp\"],len(SplittedACC)/3)\n",
    "            identifier_ = np.repeat(data2.iloc[i][\"tag-local-identifier\"],len(SplittedACC)/3)\n",
    "            X_raw_ = SplittedACC[0:len(SplittedACC):3]\n",
    "            Y_raw_ = SplittedACC[1:len(SplittedACC):3]\n",
    "            Z_raw_ = SplittedACC[2:len(SplittedACC):3]\n",
    "            ACC_num_ = [*range(0+1,round(len(SplittedACC)/3)+1)]\n",
    "\n",
    "            timestamp__ += timestamp_.tolist()\n",
    "            identifier__ += identifier_.tolist()\n",
    "            X_raw__ += X_raw_\n",
    "            Y_raw__ += Y_raw_\n",
    "            Z_raw__ += Z_raw_\n",
    "            ACC_num__ += ACC_num_\n",
    "            \n",
    "        timestamp += timestamp__\n",
    "        identifier += identifier__\n",
    "        X_raw += X_raw__\n",
    "        Y_raw += Y_raw__\n",
    "        Z_raw += Z_raw__\n",
    "        ACC_num += ACC_num__\n",
    "\n",
    "    # Combine the list into a data frame\n",
    "    data = pd.DataFrame({\"timestamp\":timestamp,\"tag-local-identifier\":identifier,\"X_raw\":X_raw,\"Y_raw\":Y_raw,\"Z_raw\":Z_raw,\"ACC_num\": ACC_num})\n",
    "\n",
    "    #--- Transform the values to g or m/s2 ---#\n",
    "    \n",
    "    # Calibrate ACC data # values from Andrea\n",
    "    cal_xzero = 2042\n",
    "    cal_cx = 0.0020\n",
    "    cal_yzero = 2042\n",
    "    cal_cy = 0.0020\n",
    "    cal_zzero = 2049\n",
    "    cal_cz = 0.0023\n",
    "    cal_g = 9.80665\n",
    "\n",
    "    # Convert ACC raw values to meaningful unit (m/s2)\n",
    "    data[\"X_mps\"] = [(int(i)-cal_xzero)*cal_cx*cal_g for i in data[\"X_raw\"]]\n",
    "    data[\"Y_mps\"] = [(int(i)-cal_yzero)*cal_cy*cal_g for i in data[\"Y_raw\"]]\n",
    "    data[\"Z_mps\"] = [(int(i)-cal_zzero)*cal_cz*cal_g for i in data[\"Z_raw\"]]\n",
    "    \n",
    "    # Add the aviary\n",
    "    data[\"Aviary\"] = Aviary\n",
    "    \n",
    "    # Add an individual ID\n",
    "    data[\"Individual\"] = data[\"Aviary\"].map(str)+\"_\"+data[\"tag-local-identifier\"].map(str)\n",
    "    \n",
    "    # Save the data into a new csv or pkl file\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"White Stork Affenberg releases MPIAB_ACC.csv\"\n",
    "file_name_out = \"DataAff_ACC.pkl\"\n",
    "aviary = [\"Affenberg_2019\",\"Affenberg_2020\"]\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep_acc(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"LifeTrack White Stork SW Germany Care Centre Releases_ACC.csv\"\n",
    "file_name_out = \"DataCC_ACC.pkl\"\n",
    "aviary = [\"CareCenter_2019\",\"CareCenter_2020\"]\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep_acc(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"LifeTrack White Stork SW Germany CASCB_ACC.csv\"\n",
    "file_name_out = \"DataCASCB_ACC.pkl\"\n",
    "aviary = [\"CASCB_East\",\"CASCB_West\"]\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep_acc(Released=False)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ODBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def CalculateODBA():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Round the timestamp to minutes\n",
    "    data[\"time\"] = data[\"timestamp\"].round('min')\n",
    "\n",
    "    # Make empty list to store the values in\n",
    "    Timestamp = []\n",
    "    Tag = []\n",
    "    ODBA = []\n",
    "    Ind = []\n",
    "\n",
    "    for tsp in data[\"time\"].unique():\n",
    "        \n",
    "        # Make empty list to store the values in\n",
    "        Timestamp_ = []\n",
    "        Tag_ = []\n",
    "        ODBA_ = []\n",
    "        Ind_ = []\n",
    "\n",
    "        data2 = data[data[\"time\"]==tsp]\n",
    "\n",
    "        for ind in data2[\"tag-local-identifier\"].unique():\n",
    "\n",
    "            data3 = data2[data2[\"tag-local-identifier\"]==ind]\n",
    "\n",
    "            # Calculate ODBA\n",
    "            Diff_X = abs(data3[\"X_mps\"] - mean(data3[\"X_mps\"]))\n",
    "            Diff_Y = abs(data3[\"Y_mps\"] - mean(data3[\"Y_mps\"]))\n",
    "            Diff_Z = abs(data3[\"Z_mps\"] - mean(data3[\"Z_mps\"]))\n",
    "\n",
    "            ODBA__ = mean(Diff_X)+mean(Diff_Y)+mean(Diff_Z)\n",
    "\n",
    "            Timestamp__ = data3[\"timestamp\"].unique()[0]\n",
    "            Tag__ = ind\n",
    "            Ind__ = data3[\"Individual\"].unique()[0]\n",
    "\n",
    "            # Put values into list\n",
    "            Timestamp_ += [Timestamp__]\n",
    "            Tag_ += [Tag__]\n",
    "            ODBA_ += [ODBA__]\n",
    "            Ind_ += [Ind__]\n",
    "\n",
    "        # Put values into list\n",
    "        Timestamp += Timestamp_\n",
    "        Tag += Tag_\n",
    "        ODBA += ODBA_\n",
    "        Ind += Ind_     \n",
    "\n",
    "    # Combine the lists into a dataframe\n",
    "    data = pd.DataFrame({\"timestamp\": Timestamp,\"tag-local-identifier\": Tag,\"Individual\": Ind,\"ODBA\": ODBA})\n",
    "\n",
    "    data[\"Aviary\"] = Aviary\n",
    "    data[\"Day\"] = data[\"timestamp\"].dt.date\n",
    "\n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_ACC.pkl\"\n",
    "file_name_out = \"DataAff_ODBA.pkl\"\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateODBA()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_ACC.pkl\"\n",
    "file_name_out = \"DataCC_ODBA.pkl\"\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateODBA()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_ACC.pkl\"\n",
    "file_name_out = \"DataCASCB_ODBA.pkl\"\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateODBA()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weather variables, location and flight information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function nearest\n",
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def AddVariables():\n",
    "\n",
    "    # Load ODBA data\n",
    "    data_O = pd.read_pickle(data_folder+file_name_in_ODBA)\n",
    "\n",
    "    # Load GPS data\n",
    "    data_G = pd.read_pickle(data_folder+file_name_in_GPS)\n",
    "\n",
    "    # Remove ACC data that is outside the subset of GPS data\n",
    "    data_G[\"IndDay\"] = data_G[\"tag-local-identifier\"].map(str)+\"_\"+data_G[\"Day\"].map(str)\n",
    "    data_O[\"IndDay\"] = data_O[\"tag-local-identifier\"].map(str)+\"_\"+data_O[\"Day\"].map(str)\n",
    "    data_O = data_O[data_O[\"IndDay\"].isin(data_G[\"IndDay\"])]\n",
    "\n",
    "    # Add a new column to data to enter Burst_A\n",
    "    data_O[\"Burst_A\"] = np.nan\n",
    "    \n",
    "    # Find the nearest start time for every burst and enter the start time in the new column\n",
    "    data_O.reset_index(drop=True, inplace=True)\n",
    "    data_O[\"UniqueNumber\"] = range(len(data_O))\n",
    "\n",
    "    for ind in data_G[\"tag-local-identifier\"].unique():\n",
    "\n",
    "        data_O_sub = data_O[data_O[\"tag-local-identifier\"]==ind]\n",
    "        data_G_sub = data_G[data_G[\"tag-local-identifier\"]==ind]\n",
    "        \n",
    "        for row in data_O_sub[\"UniqueNumber\"]:\n",
    "            First_timestamp = data_O.loc[data_O[\"UniqueNumber\"]==row,\"timestamp\"].unique()\n",
    "            if (First_timestamp < (min(data_G_sub[\"timestamp\"])-datetime.timedelta(minutes=10))):\n",
    "                continue\n",
    "                \n",
    "            if (First_timestamp > (max(data_G_sub[\"timestamp\"])+datetime.timedelta(minutes=10))):\n",
    "                continue\n",
    "\n",
    "            TimeSTPs = data_G_sub[data_G_sub[\"timestamp\"]<=First_timestamp[0]][\"timestamp\"].unique()\n",
    "           \n",
    "            if len(TimeSTPs)<1:\n",
    "                continue\n",
    "                \n",
    "            Nearest = nearest(items=TimeSTPs,pivot=First_timestamp)\n",
    "            Nearest = data_G_sub.loc[data_G_sub[\"timestamp\"]==Nearest,\"Burst_A\"].unique()\n",
    "            \n",
    "            if pd.isnull(Nearest):\n",
    "                continue\n",
    "                        \n",
    "            data_O.loc[data_O[\"UniqueNumber\"]==row,\"Burst_A\"] = Nearest\n",
    "\n",
    "    data_O[\"DiffTime\"]=abs((pd.to_datetime(data_O[\"Burst_A\"]) - pd.to_datetime(data_O[\"timestamp\"])).dt.total_seconds()/60)\n",
    "    \n",
    "\n",
    "    # Replace Burst_A by NA if the timedifference is more than 12 minutes\n",
    "    data_O.loc[~(data_O[\"DiffTime\"].isna()|(data_O[\"DiffTime\"]<12)),\"Burst_A\"] = np.nan\n",
    "\n",
    "    # Remove the data where Burst_A is NA\n",
    "    data_O = data_O[~(data_O[\"Burst_A\"].isna())]\n",
    "\n",
    "    # Make a table with the information from the GPS data\n",
    "    data_G2 = data_G.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        WindSupportPL_Mean = (\"WindSupport_PL\",'mean'),\n",
    "        WindDirectionPL_Mean = (\"WindDirection_PL\",'mean'),\n",
    "        TempSL_Mean = (\"Temp_SL\",'mean'),\n",
    "        WindSpeedPL_Mean = (\"WindSpeed_PL\",'mean'),\n",
    "        TimestampG_End = (\"timestamp\",'last'),\n",
    "        Lat_End = (\"location-lat\",'last'),\n",
    "        Long_End = (\"location-long\",'last')\n",
    "    )\n",
    "\n",
    "    # Summarise the behaviour (flying, gliding, climbing) and weather at the end (last 5 seconds) of the burst\n",
    "    data_G3 = data_G.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).tail(5)\n",
    "    data_G3 = data_G3.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        FlyingID = (\"FlyingID\",'mean'),\n",
    "        GlidingID = (\"GlidingID\",'mean'),\n",
    "        ClimbingID = (\"ClimbingID\",'mean'),\n",
    "        Altitude_End = (\"Altitude\",'mean'),\n",
    "        WindSupportPL_End = (\"WindSupport_PL\",'mean'),\n",
    "        WindSpeedPL_End = (\"WindSpeed_PL\",'mean'),\n",
    "        TempSL_End = (\"Temp_SL\",'mean'),\n",
    "        #TimestampG_End = (\"timestamp\",'last'),\n",
    "        ClimbingRate_End = (\"ClimbingRate\",'mean'),\n",
    "        Sink_End = (\"ClimbingRate\",'mean'),\n",
    "        GlidingSpeed_End = (\"ground-speed\",'mean'),\n",
    "        GlidingAirspeed_End = (\"AirSpeed_PL\",'mean')\n",
    "    )\n",
    "\n",
    "    # Summarise flight properties during climbing bits\n",
    "    data_G4 = data_G[~(data_G[\"ClimbingID\"].isna())]\n",
    "    data_G4 = data_G4.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        ClimbingRate_Mean = (\"ClimbingRate\",'mean')\n",
    "    )\n",
    "\n",
    "    # Summarise flight properties during climbing bits\n",
    "    data_G5 = data_G[~(data_G[\"GlidingID\"].isna())]\n",
    "    data_G5 = data_G5.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        Sink_Mean = (\"ClimbingRate\",'mean'),\n",
    "        GlidingSpeed_Mean = (\"ground-speed\",'mean'),\n",
    "        GlidingAirspeed_Mean = (\"AirSpeed_PL\",'mean')\n",
    "    )\n",
    "\n",
    "    # Summarise the data for the last gliding segment of the burst (only if the gliding segment is at the end)\n",
    "    data_G3g = data_G3[~(data_G3[\"GlidingID\"].isna())]\n",
    "    data_G3g = data_G3g[[\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"GlidingID\"]]\n",
    "    data_G[\"Burst_A\"] = data_G.Burst_A.astype('datetime64[ns]')\n",
    "    data_G3g = pd.merge(data_G,data_G3g)\n",
    "    \n",
    "    data_G3g = data_G3g.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"GlidingID\"],as_index=False).agg(\n",
    "        Sink_Segm = (\"ClimbingRate\",'mean'),\n",
    "        GlidingSpeed_Segm = (\"ground-speed\",'mean'),\n",
    "        GlidingAirspeed_Segm = (\"AirSpeed_PL\",'mean'),\n",
    "        WindSupportPL_Segm = (\"WindSupport_PL\",'mean')\n",
    "    )\n",
    "    \n",
    "    # Summarise the data for the last climbing segment of the burst (only if the climbing segment is at the end)\n",
    "    data_G3c = data_G3[~(data_G3[\"ClimbingID\"].isna())]\n",
    "    data_G3c = data_G3c[[\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"ClimbingID\"]]\n",
    "    data_G3c = pd.merge(data_G,data_G3c)\n",
    "    \n",
    "    data_G3c = data_G3c.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"ClimbingID\"],as_index=False).agg(\n",
    "        ClimbingRate_Segm = (\"ClimbingRate\",'mean')\n",
    "    )\n",
    "\n",
    "    # Add the information to the ODBA data\n",
    "    data_O[\"Burst_A\"] = data_O.Burst_A.astype('datetime64[ns]')\n",
    "\n",
    "    data_O2 = pd.merge(data_O,data_G2,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\"],how=\"outer\")\n",
    "\n",
    "    data_O2 = pd.merge(data_O2,data_G3,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G4,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G5,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G3g,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\",\"GlidingID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G3c,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\",\"ClimbingID\"],how=\"outer\")\n",
    "\n",
    "    data_O2[\"DiffTime_\"] = (data_O2[\"timestamp\"] - data_O2[\"TimestampG_End\"]).dt.total_seconds()\n",
    "\n",
    "    # Remove data that does not fit to a burst\n",
    "    data_O2 = data_O2[~(data_O2[\"DiffTime_\"].isna())]\n",
    "    data_O2 = data_O2[data_O2[\"DiffTime_\"]>=0]\n",
    "    data_O2 = data_O2[data_O2[\"DiffTime_\"]<120]\n",
    "    \n",
    "    # Add a column for climbing vs gliding\n",
    "    data_O2[\"FlightType\"] = np.nan\n",
    "    data_O2.loc[~(data_O2[\"GlidingID\"].isna()),\"FlightType\"] = \"Gliding\"\n",
    "    data_O2.loc[~(data_O2[\"ClimbingID\"].isna()),\"FlightType\"] = \"Climbing\"\n",
    "\n",
    "    #-----------------#\n",
    "    #- Save the data -#\n",
    "    #-----------------#\n",
    "    # Save the data into a new pkl file\n",
    "    data_O2.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_ODBA = \"DataAff_ODBA.pkl\"\n",
    "file_name_in_GPS = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff_ODBA_Temp.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddVariables()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_ODBA = \"DataCC_ODBA.pkl\"\n",
    "file_name_in_GPS = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC_ODBA_Temp.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddVariables()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_ODBA = \"DataCASCB_ODBA.pkl\"\n",
    "file_name_in_GPS = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB_ODBA_Temp.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddVariables()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the ACC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".pkl\")\n",
    "\n",
    "    # Load all data files for Affenberg\n",
    "    data_all = (pd.read_pickle(f) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"_ODBA_Temp\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to make sure all objects are removed afterwards\n",
    "def function_survival(Released):\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Load the release-death data\n",
    "    ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)\n",
    "        \n",
    "    # Only keep columns that are necessary for further steps in the analyses\n",
    "    columns_to_keep = [\n",
    "        \"timestamp\",\n",
    "        \"location-long\",\n",
    "        \"location-lat\",\n",
    "        \"tag-local-identifier\",\n",
    "        \"individual-local-identifier\",\n",
    "        \"gps:satellite-count\",\n",
    "        \"sensor-type\"\n",
    "    ]\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Only keep GPS data\n",
    "    data = data[data[\"sensor-type\"]==\"gps\"]\n",
    "\n",
    "    # Only keep locations that are not NA\n",
    "    data.dropna(subset=[\"location-long\",\"location-lat\"],inplace=True)\n",
    "\n",
    "    # Convert the timestamps\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    # Remove data before release and after death\n",
    "    \n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "   \n",
    "    # Remove duplicates\n",
    "\n",
    "    # Sort the data\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\",\"gps:satellite-count\"], ascending=[True,True,True], inplace=True)\n",
    "\n",
    "    # Remove duplicated timestamps\n",
    "    data.drop_duplicates([\"timestamp\",\"tag-local-identifier\"],keep=\"first\",inplace=True)\n",
    "\n",
    "    # Add the start-year of the data\n",
    "    Start_year = data.groupby(\"tag-local-identifier\")[\"timestamp\"].min().dt.year.reset_index()\n",
    "    data[\"StartYear\"] = data[\"tag-local-identifier\"].map(Start_year.set_index(\"tag-local-identifier\")[\"timestamp\"].to_dict())\n",
    "    \n",
    "    # Only keep individuals from 2019 and 2020\n",
    "    data = data[(data[\"StartYear\"]==2019)|(data[\"StartYear\"]==2020)]\n",
    "    \n",
    "    # Remove data from 1 July in the year of after tagging\n",
    "    data = data[data[\"timestamp\"]<[datetime.datetime(y+1,7,1) for y in data[\"StartYear\"]]]\n",
    "\n",
    "    # Set mortality after 1 July in the year after tagging to NA\n",
    "    data[\"Death2\"]=np.nan\n",
    "    Death2 = data[data[\"Death\"]<=[datetime.datetime(y+1,7,1) for y in data[\"StartYear\"]]][\"Death\"]\n",
    "    data.loc[data[\"Death\"]<=[datetime.datetime(y+1,7,1) for y in data[\"StartYear\"]],\"Death2\"] = Death2\n",
    "\n",
    "    # Add the aviary\n",
    "    data[\"Aviary\"] = Aviary\n",
    "    \n",
    "    # Add an individual ID\n",
    "    data[\"Individual\"] = data[\"Aviary\"].map(str)+\"_\"+data[\"tag-local-identifier\"].map(str)\n",
    "    \n",
    "    # Add Day without time\n",
    "    data[\"Day\"] = data[\"timestamp\"].dt.date\n",
    "    \n",
    "    # Subset the data to daily\n",
    "    data = data.groupby([\"tag-local-identifier\",\"Day\"],as_index=False).first()\n",
    "    \n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"Second_DataAff_All_Second.pkl\"\n",
    "file_name_out = \"DataAff_Survival.csv\"\n",
    "aviary = [\"Affenberg_2019\",\"Affenberg_2020\"]\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_survival(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"Second_DataCC_All_Second.pkl\"\n",
    "file_name_out = \"DataCC_Survival.csv\"\n",
    "aviary = [\"CareCenter_2019\",\"CareCenter_2020\"]\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_survival(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"Second_DataCASCB_All_Second.pkl\"\n",
    "file_name_out = \"DataCASCB_Survival.csv\"\n",
    "aviary = [\"CASCB_East\",\"CASCB_West\"]\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_survival(Released=False)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"Survival\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # days to cover segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def DaysInSegment():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Remove data that is not during flight\n",
    "    data = data[~np.isnan(data[\"FlyingID\"])]    \n",
    "\n",
    "    # Get the first and last timestamp in the data\n",
    "    data_ts = data.groupby([\"tag-local-identifier\",\"Individual\",\"Aviary\"],as_index=False).agg(\n",
    "        Timestamp_first = (\"timestamp\",'first'),\n",
    "        Timestamp_last = (\"timestamp\",'last')\n",
    "    )\n",
    "    \n",
    "    # Calculate the timedifference in days\n",
    "    data_ts[\"Days\"] = (data_ts[\"Timestamp_last\"]-data_ts[\"Timestamp_first\"]).dt.total_seconds()/(3600*24)\n",
    "    \n",
    "    # Save the data\n",
    "    data_ts.to_csv(data_folder+file_name_out+\"DaysInSegment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff\"\n",
    "Aviary=\"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DaysInSegment()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC\"\n",
    "Aviary=\"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DaysInSegment()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB\"\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DaysInSegment()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"DaysInSegment\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def DailyDist():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in_1)\n",
    "    data_s = pd.read_pickle(data_folder+file_name_in_2)\n",
    "    \n",
    "    data_s[\"ID\"] = data_s[\"tag-local-identifier\"].map(str) + data_s[\"Day\"].map(str)\n",
    "    data[\"ID\"] = data[\"tag-local-identifier\"].map(str) + data[\"Day\"].map(str)\n",
    "\n",
    "    data[\"Aviary\"] = Aviary\n",
    "    data[\"Individual\"] = data[\"Aviary\"].map(str) + data[\"tag-local-identifier\"].map(str)\n",
    "    data = data[data[\"ID\"].isin(data_s[\"ID\"])]\n",
    "\n",
    "    # Get the start and end of the daily flight\n",
    "    data2 = data.groupby([\"Day\",\"tag-local-identifier\",\"Individual\",\"Aviary\"],as_index=False).agg(\n",
    "        Lat_first = (\"location-lat\",'first'),\n",
    "        Lat_last = (\"location-lat\",'last'),\n",
    "        Long_first = (\"location-long\",'first'),\n",
    "        Long_last = (\"location-long\",'last')\n",
    "    )\n",
    "        \n",
    "    # Get the distance between the first location and the last location of the day\n",
    "    Distances = [None] * len(data2)\n",
    "    for row in range(len(data2)):\n",
    "        Distances[row] = geopy.distance.distance((data2[\"Lat_first\"].iloc()[row], data2[\"Long_first\"].iloc()[row]),(data2[\"Lat_last\"].iloc()[row], data2[\"Long_last\"].iloc()[row])).km\n",
    "    data2[\"Distance\"] = Distances\n",
    "    \n",
    "    # Save the data\n",
    "    data2.to_csv(data_folder+file_name_out+\"DailyDistance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_1 = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_in_2 = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff\"\n",
    "Aviary=\"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DailyDist()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_1 = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_in_2 = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC\"\n",
    "Aviary=\"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DailyDist()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_1 = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_in_2 = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB\"\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DailyDist()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"DailyDistance\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route straightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def EfficiencySegment():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Remove data that is not during flight\n",
    "    data = data[~np.isnan(data[\"FlyingID\"])]    \n",
    "\n",
    "    # Get the start and end of the daily flight\n",
    "    data_dist = data.groupby([\"tag-local-identifier\",\"Individual\",\"Aviary\"],as_index=False).agg(\n",
    "        Lat_first = (\"location-lat\",'first'),\n",
    "        Lat_last = (\"location-lat\",'last'),\n",
    "        Long_first = (\"location-long\",'first'),\n",
    "        Long_last = (\"location-long\",'last')\n",
    "    )\n",
    "    \n",
    "    #-------------------------------#\n",
    "    #- Straight distance - segment -#\n",
    "    #-------------------------------#\n",
    "    \n",
    "    # Calculate the distance between the first and last location for every individual\n",
    "    Distances = [None] * len(data_dist)\n",
    "    for row in range(len(data_dist)):\n",
    "        Distances[row] = geopy.distance.distance((data_dist[\"Lat_first\"].iloc()[row], data_dist[\"Long_first\"].iloc()[row]),(data_dist[\"Lat_last\"].iloc()[row], data_dist[\"Long_last\"].iloc()[row])).km\n",
    "    data_dist[\"Dist_Segm\"] = Distances\n",
    "    \n",
    "    #---------------------------------------#\n",
    "    #- Cumulative distance - segment - all -#\n",
    "    #---------------------------------------#\n",
    "    \n",
    "    # Calculate the cumulative distance for every individual using all data\n",
    "    \n",
    "    # Make an empty list to enter the values\n",
    "    Distances = [None] * len(data_dist)\n",
    "    \n",
    "    # Calculate the distance between the first and last location for each individual\n",
    "    for row in range(len(data_dist)):\n",
    "        \n",
    "        # Subset the data so that in only contains the given individual\n",
    "        data_ind = data[data[\"tag-local-identifier\"]==data_dist[\"tag-local-identifier\"].iloc()[row]]\n",
    "\n",
    "        # Create an empty list\n",
    "        Distancess = [None] * (len(data_ind)-1)\n",
    "        \n",
    "        # Calculate the distance between each location and the location in the next row\n",
    "        for rows in range(len(data_ind)-1):\n",
    "            Distancess[rows] = geopy.distance.distance((data_ind[\"location-lat\"].iloc()[rows],data_ind[\"location-long\"].iloc()[rows]),(data_ind[\"location-lat\"].iloc()[rows+1],data_ind[\"location-long\"].iloc()[rows+1])).km\n",
    "\n",
    "        # Calculate the cumulative distance for the individual\n",
    "        Distances[row] = sum(Distancess)\n",
    "        \n",
    "    # Add the cumulative distances to the data\n",
    "    data_dist[\"CumDist_Segm_All\"] = Distances\n",
    "    \n",
    "    # Calculate the straightness\n",
    "    data_dist[\"Straightness_Segm\"] = data_dist[\"Dist_Segm\"]/data_dist[\"CumDist_Segm_All\"]\n",
    "    \n",
    "    # Save the data\n",
    "    data_dist.to_csv(data_folder+file_name_out+\"EfficiencySegment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff\"\n",
    "Aviary=\"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "EfficiencySegment()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC\"\n",
    "Aviary=\"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "EfficiencySegment()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB\"\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "EfficiencySegment()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"EfficiencySegment\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily flight time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that finds the start and end of migration flights\n",
    "\n",
    "def FlightTime():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Get the start and end of the daily flight\n",
    "    data = data.groupby([\"Day\",\"tag-local-identifier\",\"Individual\",\"Aviary\"],as_index=False).agg(\n",
    "        StartFlight = (\"StartFlight\",'first'),\n",
    "        EndFlight = (\"EndFlight\",'first'),\n",
    "        DayLength = (\"DayLength\",'mean')\n",
    "    )\n",
    "    \n",
    "    # Get the number of hours the storks have been flying (end time - start time)\n",
    "    data[\"DailyFlightTime\"] = (data[\"EndFlight\"] - data[\"StartFlight\"]).dt.total_seconds()/3600\n",
    "    \n",
    "    # Get the day length in hours\n",
    "    data[\"DayLength\"] = data[\"DayLength\"]/60\n",
    "    \n",
    "    # Save the data\n",
    "    data.to_csv(data_folder+file_name_out+\"DailyFlightTime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FlightTime()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FlightTime()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "FlightTime()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"DailyFlightTime\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily cross country speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def CrossCountrySpeedDay():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Get a data frame with the first and last timestamps and locations for every burst\n",
    "    #    And with the average U and V wind component at pressure level\n",
    "    data_2 = data.groupby([\"Day\",\"tag-local-identifier\",\"Individual\",\"Aviary\"],as_index=False).agg(\n",
    "        StartLat = (\"location-lat\",'first'),\n",
    "        EndLat = (\"location-lat\",'last'),\n",
    "        StartLong = (\"location-long\",'first'),\n",
    "        EndLong = (\"location-long\",'last'),\n",
    "        StartTime = (\"timestamp\",'first'),\n",
    "        EndTime = (\"timestamp\",'last'),\n",
    "        U_Wind_PL_Mean=(\"U_Wind_PL\",'mean'),\n",
    "        V_Wind_PL_Mean=(\"V_Wind_PL\",'mean')\n",
    "    )\n",
    "\n",
    "    # Define a list to enter all distances\n",
    "    Distances = [None] * len(data_2)\n",
    "\n",
    "    # Calculate the straight-line distance between the locations for each row in the data\n",
    "    for row in range(len(data_2)):\n",
    "        Distances[row] = geopy.distance.distance((data_2[\"StartLat\"].iloc()[row],data_2[\"StartLong\"].iloc()[row]),(data_2[\"EndLat\"].iloc()[row],data_2[\"EndLong\"].iloc()[row])).km\n",
    "\n",
    "    # Enter the distances in the data frame\n",
    "    data_2[\"StraightDist\"] = Distances\n",
    "\n",
    "    # Define a list to enter all distances\n",
    "    Distances = [None] * len(data_2)\n",
    "\n",
    "    # Calculate the cumulative distance between the locations for each row in the data\n",
    "    for row in range(len(data_2)):\n",
    "        data_3 = data[(data.Day==data_2[\"Day\"].iloc()[row])&(data[\"tag-local-identifier\"]==data_2[\"tag-local-identifier\"].iloc()[row])]\n",
    "\n",
    "        Distancess = [None] * (len(data_3)-1)\n",
    "        for rows in range(len(data_3)-1):\n",
    "            Distancess[rows] = geopy.distance.distance((data_3[\"location-lat\"].iloc()[rows],data_3[\"location-long\"].iloc()[rows]),(data_3[\"location-lat\"].iloc()[rows+1],data_3[\"location-long\"].iloc()[rows+1])).km\n",
    "\n",
    "        Distances[row] = sum(Distancess)\n",
    "\n",
    "    # Enter the distances in the data frame\n",
    "    data_2[\"CumDist\"] = Distances\n",
    "    \n",
    "    # Calculate the time difference in seconds\n",
    "    data_2[\"DiffTime\"] = (data_2[\"EndTime\"]-data_2[\"StartTime\"]).dt.total_seconds()\n",
    "    \n",
    "\n",
    "    # Calculate wind support\n",
    "    \n",
    "    # Calculate wind speed and direction\n",
    "    \n",
    "    # The U component of the wind is the west-east component\n",
    "    # The V component of the wind is the south-north component\n",
    "\n",
    "    # Calculate horizontal windspeed\n",
    "    data_2[\"WindSpeed_PL\"] = (data_2[\"U_Wind_PL_Mean\"]**2 + data_2[\"V_Wind_PL_Mean\"]**2)**0.5\n",
    "    \n",
    "    # Calculate wind direction\n",
    "    # Formula from https://www.eol.ucar.edu/content/wind-direction-quick-reference\n",
    "    # arctan2 calculates the direction from which the wind comes. +180 -> wind direction\n",
    "    data_2[\"WindDirection_PL\"] = np.arctan2(-data_2[\"U_Wind_PL_Mean\"],-data_2[\"V_Wind_PL_Mean\"])*(180/np.pi)+180\n",
    "    \n",
    "\n",
    "    # Make all angles smaller than 360\n",
    "    if len(data_2.loc[data_2[\"WindDirection_PL\"]>360])>0:\n",
    "        print(\"Some angles are larger than 360\")\n",
    "        data.loc[data_2[\"WindDirection_PL\"]>360] = data_2.loc[data_2[\"WindDirection_PL\"]>360] - 360 # Not tested, because this line is most likely not necessary   \n",
    "    \n",
    "    # Calculate the overall heading of the stork\n",
    "    Headings = [None] * len(data_2)\n",
    "    for row in range(len(data_2)):\n",
    "        X_diff = data_2[\"EndLong\"].iloc()[row]-data_2[\"StartLong\"].iloc()[row]\n",
    "        Y_diff = data_2[\"EndLat\"].iloc()[row]-data_2[\"StartLat\"].iloc()[row]\n",
    "        Headings[row] = np.arctan2(-X_diff,-Y_diff)*(180/np.pi)+180\n",
    "    \n",
    "\n",
    "    data_2[\"heading\"] = Headings\n",
    "    \n",
    "    # Calculate windsupport and crosswind\n",
    "    \n",
    "    # Calculate the angle between the direction of the stork (heading) and the direction of the wind\n",
    "    data_2[\"Angle_heading_WindPL\"] = data_2[\"heading\"] - data_2[\"WindDirection_PL\"]\n",
    "    \n",
    "    # Determine the a-angle to do the calculations with\n",
    "    data_2[\"Angle_a\"] = abs(data_2[\"Angle_heading_WindPL\"])\n",
    "    \n",
    "    # Calculate the w-angle\n",
    "    data_2[\"Angle_w\"] = 90-data_2[\"Angle_a\"]\n",
    "\n",
    "    # Calculate windsupport\n",
    "    # Formula from https://doi.org/10.1186/2051-3933-1-4\n",
    "    # Windsupport is in the direction of the heading\n",
    "    data_2[\"WindSupport_PL\"] = data_2[\"WindSpeed_PL\"]*np.sin(data_2[\"Angle_w\"]*np.pi/180)\n",
    "    \n",
    "\n",
    "    # Calculate crosswind\n",
    "    # Formula from https://doi.org/10.1186/2051-3933-1-4\n",
    "    data_2[\"CrossWind_PL\"] = data_2[\"WindSpeed_PL\"]*np.cos(data_2[\"Angle_w\"]*np.pi/180)\n",
    "        \n",
    "\n",
    "    # Save the data\n",
    "    data_2.to_csv(data_folder+file_name_out+\"CrossCountrySpeedDay.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CrossCountrySpeedDay()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CrossCountrySpeedDay()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CrossCountrySpeedDay()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"CrossCountrySpeedDay\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average climbing and gliding properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def CalculateAVGClimbingGliding():\n",
    "\n",
    "    #---------------------------------#\n",
    "    #- Preparation of the data frame -#\n",
    "    #---------------------------------#\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    #----------------#\n",
    "    #- Calculations -#\n",
    "    #----------------#\n",
    "    \n",
    "    # Calculate the averages per burst\n",
    "    avg_ClimbingB = data[~data.ClimbingID.isna()].groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\"],as_index=False).agg(\n",
    "        Mean_ClimbingRate = (\"ClimbingRate\",'mean'),\n",
    "        N_Locations = (\"ClimbingRate\",'count'),\n",
    "        Min_Timestamp = (\"timestamp\", 'min'),\n",
    "        Max_Timestamp = (\"timestamp\",'max'),\n",
    "        SD_ClimbingRate = (\"ClimbingRate\",'std'),\n",
    "        Mean_ClimbingSpeed = (\"ground-speed\",'mean'),\n",
    "        SD_ClimbingSpeed = (\"ground-speed\",'std'),\n",
    "        Mean_TempSL = (\"Temp_SL\",'mean'),\n",
    "        SD_TempSL = (\"Temp_SL\",'std'),\n",
    "        Mean_Elevation = (\"Elevation\",'mean'),\n",
    "        SD_Elevation = (\"Elevation\",'std'),\n",
    "        Last_Elevation = (\"Elevation\",'last'),\n",
    "        First_Elevation = (\"Elevation\",'first'),\n",
    "        Mean_Altitude = (\"Altitude\",'mean'),\n",
    "        SD_Altitude = (\"Altitude\",'std'),\n",
    "        Last_Altitude = (\"Altitude\",'last'),\n",
    "        First_Altitude = (\"Altitude\",'first'),\n",
    "        Mean_WindSpeedPL = (\"WindSpeed_PL\",'mean'),\n",
    "        SD_WindSpeedPL = (\"WindSpeed_PL\",'std'),\n",
    "        Mean_WindSupportPL = (\"WindSupport_PL\",'mean'),\n",
    "        SD_WindSupportPL = (\"WindSupport_PL\",'std'),\n",
    "        Mean_AirSpeedPL = (\"AirSpeed_PL\",'mean'),\n",
    "        SD_AirSpeedPL = (\"AirSpeed_PL\",'std'),\n",
    "        Mean_Heading = (\"heading\",'mean'),\n",
    "        SD_Heading = (\"heading\",'std'),\n",
    "        Mean_WindDirectionPL = (\"WindDirection_PL\",'mean'),\n",
    "        SE_WindDirectionPL = (\"WindDirection_PL\",'mean')\n",
    "    )\n",
    "    \n",
    "    avg_GlidingB = data[~data.GlidingID.isna()].groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\"],as_index=False).agg(\n",
    "        Mean_SinkingSpeed = (\"ClimbingRate\",'mean'),\n",
    "        N_Locations = (\"ClimbingRate\",'count'),\n",
    "        Min_Timestamp = (\"timestamp\", 'min'),\n",
    "        Max_Timestamp = (\"timestamp\",'max'),\n",
    "        SD_SinkingSpeed = (\"ClimbingRate\",'std'),\n",
    "        Mean_GlidingSpeed = (\"ground-speed\",'mean'),\n",
    "        SD_GlidingSpeed = (\"ground-speed\",'std'),\n",
    "        Mean_TempSL = (\"Temp_SL\",'mean'),\n",
    "        SD_TempSL = (\"Temp_SL\",'std'),\n",
    "        Mean_Elevation = (\"Elevation\",'mean'),\n",
    "        SD_Elevation = (\"Elevation\",'std'),\n",
    "        Last_Elevation = (\"Elevation\",'last'),\n",
    "        First_Elevation = (\"Elevation\",'first'),\n",
    "        Mean_Altitude = (\"Altitude\",'mean'),\n",
    "        SD_Altitude = (\"Altitude\",'std'),\n",
    "        Last_Altitude = (\"Altitude\",'last'),\n",
    "        First_Altitude = (\"Altitude\",'first'),\n",
    "        Mean_WindSpeedPL = (\"WindSpeed_PL\",'mean'),\n",
    "        SD_WindSpeedPL = (\"WindSpeed_PL\",'std'),\n",
    "        Mean_WindSupportPL = (\"WindSupport_PL\",'mean'),\n",
    "        SD_WindSupportPL = (\"WindSupport_PL\",'std'),\n",
    "        Mean_AirSpeedPL = (\"AirSpeed_PL\",'mean'),\n",
    "        SD_AirSpeedPL = (\"AirSpeed_PL\",'std'),\n",
    "        Mean_Heading = (\"heading\",'mean'),\n",
    "        SD_Heading = (\"heading\",'std'),\n",
    "        Mean_WindDirectionPL = (\"WindDirection_PL\",'mean'),\n",
    "        SE_WindDirectionPL = (\"WindDirection_PL\",'mean')\n",
    "    )\n",
    "    \n",
    "    # Save the data\n",
    "    avg_ClimbingB.to_csv(data_folder+file_name_out+\"_avgClimbingB.csv\")\n",
    "    avg_GlidingB.to_csv(data_folder+file_name_out+\"_avgGlidingB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateAVGClimbingGliding()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateAVGClimbingGliding()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateAVGClimbingGliding()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files for Affenberg\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"avgClimbingB\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"avgGlidingB\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altitude at which storks leave thermals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def AltThermal():\n",
    "\n",
    "    #---------------------------------#\n",
    "    #- Preparation of the data frame -#\n",
    "    #---------------------------------#\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    #----------------#\n",
    "    #- Calculations -#\n",
    "    #----------------#\n",
    "\n",
    "    # Find the altitude at the end of climbing segments\n",
    "    last_Altitude = data.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"ClimbingID\"],as_index=False).agg(\n",
    "        Min_Timestamp = (\"timestamp\",'min'),\n",
    "        Max_Timestamp = (\"timestamp\",'max'),\n",
    "        Last_Altitude = (\"Altitude\",'last'),\n",
    "        First_Altitude = (\"Altitude\",'first')\n",
    "    )\n",
    "\n",
    "    # Get the time at the start and end of each burst\n",
    "    BurstStartEnds = data.groupby([\"BurstID\"],as_index=False).agg(\n",
    "        StartBurst = (\"timestamp\",'min'),\n",
    "        EndBurst = (\"timestamp\",'max')\n",
    "    )\n",
    "\n",
    "    # Add the start and end times of the burst to the data \n",
    "    last_Altitude = last_Altitude.merge(BurstStartEnds,on=[\"BurstID\"],how=\"left\")   \n",
    "\n",
    "    # Remove the gliding segments at the end of the bursts\n",
    "    last_Altitude = last_Altitude[last_Altitude[\"Max_Timestamp\"]<=(last_Altitude[\"EndBurst\"]+datetime.timedelta(seconds=5))]\n",
    "    \n",
    "    #-------------#\n",
    "    #- Save data -#\n",
    "    #-------------#\n",
    "    \n",
    "    # Save the data\n",
    "    last_Altitude.to_csv(data_folder+file_name_out+\"_LastAltitude.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AltThermal()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AltThermal()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AltThermal()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"LastAltitude\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional calculations for supplementary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration timing west of Black Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function\n",
    "def TimingCC():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "    \n",
    "    # Filter the data\n",
    "    ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)\n",
    "    data[\"Aviary2\"] = data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Aviary\"].to_dict())\n",
    "    data.dropna(subset=[\"Aviary2\"],inplace=True)\n",
    "\n",
    "    # Group the data by day and get first timestamp\n",
    "    data = data.groupby([\"Aviary\",\"tag-local-identifier\",\"Individual\"],as_index=False).agg(\n",
    "        Segment = (\"timestamp\",'first')\n",
    "    )\n",
    "    \n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC_MigTimingCC.csv\"\n",
    "aviary = [\"CareCenter_2019\",\"CareCenter_2020\"]\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "TimingCC()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB_MigTimingCC.csv\"\n",
    "aviary = [\"CASCB_West\"]\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "TimingCC()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"MigTimingCC\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### # days between release and migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function\n",
    "def DaysRelMig():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Group the data by day and get first and last latitude and longitude\n",
    "    data = data.groupby([\"Aviary\",\"tag-local-identifier\",\"Individual\"],as_index=False).agg(\n",
    "        Release = (\"Release\",'first'),\n",
    "        Segment = (\"timestamp\",'first')\n",
    "    )\n",
    "    \n",
    "    # Calculate the time difference in days\n",
    "    data[\"TimeDiff\"] = data[\"Segment\"]-data[\"Release\"]\n",
    "    data[\"DayDiff\"] = (data[\"Segment\"]-data[\"Release\"]).dt.round('d').dt.days\n",
    "    \n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff_DaysRelMig.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DaysRelMig()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC_DaysRelMig.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DaysRelMig()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"DaysRelMig\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily distance and southward displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function\n",
    "def DailyDistDisp():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Group the data by day and get first and last latitude and longitude\n",
    "    data = data.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\"],as_index=False).agg(\n",
    "        FirstLat = (\"location-lat\",'first'),\n",
    "        LastLat = (\"location-lat\",'last'),\n",
    "        FirstLong = (\"location-long\",'first'),\n",
    "        LastLong = (\"location-long\",'last')\n",
    "    )\n",
    "\n",
    "    # Calculate daily distance\n",
    "    Dist = [None] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Dist[i] = geopy.distance.distance((data[\"FirstLat\"][i],data[\"FirstLong\"][i]),(data[\"LastLat\"][i],data[\"LastLong\"][i])).km\n",
    "\n",
    "    data[\"Dist\"] = Dist\n",
    "\n",
    "    # Calculate daily latitudinal displacement\n",
    "    data[\"Disp\"] = data[\"FirstLat\"] - data[\"LastLat\"]\n",
    "    \n",
    "    # Calculate daily southward displacement\n",
    "    Dist2 = [None] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        MeanLong = (data[\"FirstLong\"][i] + data[\"LastLong\"][i])/2\n",
    "        Dist2[i] = geopy.distance.distance((data[\"FirstLat\"][i],MeanLong),(data[\"LastLat\"][i],MeanLong)).km\n",
    "\n",
    "    data[\"DistSouth\"] = Dist2\n",
    "    \n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_out = \"DataAff_DailyDistDisp.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DailyDistDisp()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCC_DailyDistDisp.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DailyDistDisp()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCASCB_DailyDistDisp.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "DailyDistDisp()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"DailyDistDisp\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thermalling conditions in Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Affenberg storks stop around 41.7 degrees latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function\n",
    "def ThermallingSpain():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Keep only data during flight (the main migration flight)\n",
    "    data = data[(~data.StartFlight.isna())&(~data.EndFlight.isna())]\n",
    "    data = data[data.timestamp>=data.StartFlight]\n",
    "    data = data[data.timestamp<(data.EndFlight+datetime.timedelta(minutes=5))]\n",
    "\n",
    "    # Keep only migration days (>50 km)\n",
    "    Subset = data.groupby([\"Day\",\"tag-local-identifier\"],as_index=False)[[\"location-long\",\"location-lat\"]].agg(['first','last'])\n",
    "    Subset.reset_index(level=0, inplace=True)\n",
    "    Subset.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    Dist = [None] * len(Subset)\n",
    "    for i in range(len(Subset)):\n",
    "        Dist[i] = geopy.distance.distance((Subset[\"location-lat\"][\"first\"][i],Subset[\"location-long\"][\"first\"][i]),(Subset[\"location-lat\"][\"last\"][i],Subset[\"location-long\"][\"last\"][i])).km\n",
    "\n",
    "    Subset[\"Dist\"] = Dist\n",
    "    Subset = Subset[Subset[\"Dist\"]>=50]\n",
    "\n",
    "    data[\"MigDay\"] = False\n",
    "    for i in data[\"tag-local-identifier\"].unique():\n",
    "        data.loc[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"].isin(Subset[Subset[\"tag-local-identifier\"]==i][\"Day\"])),[\"MigDay\"]] = True\n",
    "        \n",
    "    data = data[data[\"MigDay\"]]\n",
    "    \n",
    "    # Summarise the data\n",
    "    data = data[~np.isnan(data[\"ClimbingID\"])]\n",
    "    data = data.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\"],as_index=False).agg(\n",
    "        Min_Timestamp = (\"timestamp\",'min'),\n",
    "        First_Latitude = (\"location-lat\",'first'),\n",
    "        Mean_ClimbingRate = (\"ClimbingRate\",'mean')\n",
    "    )\n",
    "    \n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_out = \"DataAff_ThermallingSpain.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "ThermallingSpain()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCC_ThermallingSpain.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "ThermallingSpain()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCASCB_ThermallingSpain.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "ThermallingSpain()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"ThermallingSpain\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function\n",
    "def ThermallingSpain_burst():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Keep only data during flight (the main migration flight)\n",
    "    data = data[(~data.StartFlight.isna())&(~data.EndFlight.isna())]\n",
    "    data = data[data.timestamp>=data.StartFlight]\n",
    "    data = data[data.timestamp<(data.EndFlight+datetime.timedelta(minutes=5))]\n",
    "\n",
    "    # Keep only migration days (>50 km)\n",
    "    Subset = data.groupby([\"Day\",\"tag-local-identifier\"],as_index=False)[[\"location-long\",\"location-lat\"]].agg(['first','last'])\n",
    "    Subset.reset_index(level=0, inplace=True)\n",
    "    Subset.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    Dist = [None] * len(Subset)\n",
    "    for i in range(len(Subset)):\n",
    "        Dist[i] = geopy.distance.distance((Subset[\"location-lat\"][\"first\"][i],Subset[\"location-long\"][\"first\"][i]),(Subset[\"location-lat\"][\"last\"][i],Subset[\"location-long\"][\"last\"][i])).km\n",
    "\n",
    "    Subset[\"Dist\"] = Dist\n",
    "    Subset = Subset[Subset[\"Dist\"]>=50]\n",
    "\n",
    "    data[\"MigDay\"] = False\n",
    "    for i in data[\"tag-local-identifier\"].unique():\n",
    "        data.loc[(data[\"tag-local-identifier\"]==i)&(data[\"Day\"].isin(Subset[Subset[\"tag-local-identifier\"]==i][\"Day\"])),[\"MigDay\"]] = True\n",
    "        \n",
    "    data = data[data[\"MigDay\"]]\n",
    "    \n",
    "    # Summarise the data\n",
    "    data = data[~np.isnan(data[\"ClimbingID\"])]\n",
    "    data = data.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\"],as_index=False).agg(\n",
    "        Min_Timestamp = (\"timestamp\",'min'),\n",
    "        First_Latitude = (\"location-lat\",'first'),\n",
    "        Mean_ClimbingRate = (\"ClimbingRate\",'mean')\n",
    "    )\n",
    "    \n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_TempWind_R.pkl\"\n",
    "file_name_out = \"DataAff_ThermalBurstSpain.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "ThermallingSpain_burst()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCC_ThermalBurstSpain.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "ThermallingSpain_burst()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_TempWind_R.pkl\"\n",
    "file_name_out = \"DataCASCB_ThermalBurstSpain.csv\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "ThermallingSpain_burst()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".csv\")\n",
    "\n",
    "    # Load all data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All_\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"ThermalBurstSpain\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data to annotate for general patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll2(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".pkl\")\n",
    "\n",
    "    # Load all data files for Affenberg\n",
    "    data_all = (pd.read_pickle(f) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_pickle(data_folder+\"All_\"+pattern+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll2(pattern=\"TempWind_S\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first and last date on which storks were in the segment\n",
    "data_ = pd.read_pickle(data_folder+\"All_TempWind_S.pkl\")\n",
    "\n",
    "print(\"First date 2019: \",data_[data_[\"StartYear\"]==2019][\"timestamp\"].min())\n",
    "print(\"First date 2020: \",data_[data_[\"StartYear\"]==2020][\"timestamp\"].min())\n",
    "print(\"Last date 2019: \",data_[data_[\"StartYear\"]==2019][\"timestamp\"].max())\n",
    "print(\"Last date 2020: \",data_[data_[\"StartYear\"]==2020][\"timestamp\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first and last date on which storks from specific studies were in the segment\n",
    "print(\"First date 2019 Aff: \",data_[(data_[\"StartYear\"]==2019)&(data_[\"Aviary\"]==\"Affenberg\")][\"timestamp\"].min())\n",
    "print(\"First date 2020 Aff: \",data_[(data_[\"StartYear\"]==2020)&(data_[\"Aviary\"]==\"Affenberg\")][\"timestamp\"].min())\n",
    "print(\"Last date 2019 Aff: \",data_[(data_[\"StartYear\"]==2019)&(data_[\"Aviary\"]==\"Affenberg\")][\"timestamp\"].max())\n",
    "print(\"Last date 2020 Aff: \",data_[(data_[\"StartYear\"]==2020)&(data_[\"Aviary\"]==\"Affenberg\")][\"timestamp\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First date 2020 CASCB: \",data_[(data_[\"StartYear\"]==2020)&(data_[\"Aviary\"]==\"CASCB\")][\"timestamp\"].min())\n",
    "print(\"Last date 2020 CASCB: \",data_[(data_[\"StartYear\"]==2020)&(data_[\"Aviary\"]==\"CASCB\")][\"timestamp\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make files for 2019 and 2020 with the track of single storks within the segment for the date sequence between 29 July and 25 September\n",
    "\n",
    "#-------------#\n",
    "#- Affenberg -#\n",
    "#-------------#\n",
    "\n",
    "#-------------------------#\n",
    "#--- 2019 - stork 7017 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataAff_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7017]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2019\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2019,7,29)-datetime.timedelta(days=1),datetime.datetime(2019,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2019_stork7017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2020 - stork 7017 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataAff_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7017]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2020\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2020,7,29)-datetime.timedelta(days=1),datetime.datetime(2020,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2020_stork7017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2019 - stork 7026 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataAff_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7026]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2019\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2019,7,29)-datetime.timedelta(days=1),datetime.datetime(2019,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2019_stork7026.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2020 - stork 7026 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataAff_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7026]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2020\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2020,7,29)-datetime.timedelta(days=1),datetime.datetime(2020,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2020_stork7026.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2019 - stork 7988 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataAff_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7988]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2019\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2019,7,29)-datetime.timedelta(days=1),datetime.datetime(2019,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2019_stork7988.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2020 - stork 7988 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataAff_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7988]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2020\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2020,7,29)-datetime.timedelta(days=1),datetime.datetime(2020,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2020_stork7988.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------#\n",
    "#- CASCB -#\n",
    "#---------#\n",
    "\n",
    "#-------------------------#\n",
    "#--- 2019 - stork 8030 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataCASCB_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==8030]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2019\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2019,7,29)-datetime.timedelta(days=1),datetime.datetime(2019,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2019_stork8030.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2020 - stork 8030 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataCASCB_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==8030]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2020\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2020,7,29)-datetime.timedelta(days=1),datetime.datetime(2020,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2020_stork8030.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2019 - stork 7999 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataCASCB_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7999]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2019\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2019,7,29)-datetime.timedelta(days=1),datetime.datetime(2019,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2019_stork7999.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2020 - stork 7999 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataCASCB_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==7999]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2020\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2020,7,29)-datetime.timedelta(days=1),datetime.datetime(2020,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2020_stork7999.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2019 - stork 8046 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataCASCB_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==8046]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2019\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2019,7,29)-datetime.timedelta(days=1),datetime.datetime(2019,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2019_stork8046.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------#\n",
    "#--- 2020 - stork 8046 ---#\n",
    "#-------------------------#\n",
    "\n",
    "# Load data\n",
    "data = pd.read_pickle(data_folder+\"DataCASCB_TempWind_S.pkl\")\n",
    "\n",
    "# Subset the data to only contain one stork\n",
    "data = data[data[\"tag-local-identifier\"]==8046]\n",
    "\n",
    "# Remove burst data\n",
    "data[\"TimeDifference\"] = data[\"timestamp\"].diff().dt.total_seconds()\n",
    "data = data[data[\"TimeDifference\"]>1]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data[[\"timestamp\",\"location-long\",\"location-lat\",\"height-above-ellipsoid\",\"Aviary\",\"heading\"]]\n",
    "\n",
    "# Add the year\n",
    "data[\"Year\"] = 2020\n",
    "\n",
    "# Make a sequence for every date in between 29-7 and 25-9 for both 2019 and 2020\n",
    "DateSeq = pd.date_range(datetime.datetime(2020,7,29)-datetime.timedelta(days=1),datetime.datetime(2020,9,25)+datetime.timedelta(days=1),freq=\"D\")\n",
    "\n",
    "# Calculate how many days the timestamp is from the minimum timestamp\n",
    "data[\"DaysDiff\"] = (data[\"timestamp\"] - min(data[\"timestamp\"])).dt.days\n",
    "\n",
    "# Now, change the timestamps\n",
    "data[\"timestamp_O\"] = data[\"timestamp\"]\n",
    "data[\"timestamp\"] = [datetime.datetime.combine(DateSeq[0]+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the data into a separate object and set the timestamp back\n",
    "data_append = data.copy()\n",
    "data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "for i in DateSeq[1:]:\n",
    "    \n",
    "    # Change the timestamp\n",
    "    data[\"timestamp\"] = [datetime.datetime.combine(i+datetime.timedelta(days=int(data[\"DaysDiff\"].iloc[row])),data[\"timestamp\"].dt.time.iloc[row]) for row in range(len(data))]\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Append the data\n",
    "    data_append = data_append.append(data).copy()\n",
    "    \n",
    "    # Change the timestamp back\n",
    "    data[\"timestamp\"] = data[\"timestamp_O\"]\n",
    "\n",
    "# Save the data\n",
    "data_append.to_csv(data_folder+\"TestWindDiff/TestWindDiff_2020_stork8046.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General wind pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeWindData():\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"TestWindDiff/TestWindDiff_\"+(\"[0-9]\"*4)+\"_stork\"+(\"[0-9]\"*4)+\".csv-\"+\"*\"+\".csv\")\n",
    "    \n",
    "    # Load the data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new file\n",
    "    data.to_pickle(data_folder+\"TestWindDiff/TestWindDiff_All.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "MergeWindData()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def CalculateWind():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+\"TestWindDiff/TestWindDiff_All.pkl\")\n",
    "    \n",
    "    # Rename columns\n",
    "    data.rename({\n",
    "        \"ECMWF ERA5 PL U Wind\":\"U_Wind_PL\",\n",
    "        \"ECMWF ERA5 PL V Wind\":\"V_Wind_PL\"\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    # Convert timestamps\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    #--------------------------------------#\n",
    "    #- Calculate wind speed and direction -#\n",
    "    #--------------------------------------#\n",
    "    \n",
    "    # The U component of the wind is the west-east component\n",
    "    # The V component of the wind is the south-north component\n",
    "\n",
    "    # Calculate horizontal windspeed\n",
    "    data[\"WindSpeed_PL\"] = (data[\"U_Wind_PL\"]**2 + data[\"V_Wind_PL\"]**2)**0.5\n",
    "\n",
    "    # Calculate wind direction\n",
    "    # Formula from https://www.eol.ucar.edu/content/wind-direction-quick-reference\n",
    "    # arctan2 calculates the direction from which the wind comes. +180 -> wind direction\n",
    "    data[\"WindDirection_PL\"] = np.arctan2(-data[\"U_Wind_PL\"],-data[\"V_Wind_PL\"])*(180/np.pi)+180\n",
    "    \n",
    "    # Make all angles smaller than 360\n",
    "    if len(data.loc[data[\"WindDirection_PL\"]>360])>0:\n",
    "        print(\"Some angles are larger than 360\")\n",
    "        data.loc[data[\"WindDirection_PL\"]>360] = data.loc[data[\"WindDirection_PL\"]>360] - 360 # Not tested, because this line is most likely not necessary\n",
    "        \n",
    "    #---------------------------------------#\n",
    "    #- Calculate windsupport and crosswind -#\n",
    "    #---------------------------------------#\n",
    "\n",
    "    # Calculate the angle between the direction of the stork (heading) and the direction of the wind\n",
    "    data[\"Angle_heading_WindPL\"] = data[\"heading\"] - data[\"WindDirection_PL\"]\n",
    "\n",
    "    # Determine the a-angle to do the calculations with\n",
    "    data[\"Angle_a\"] = abs(data[\"Angle_heading_WindPL\"])\n",
    "\n",
    "    # Calculate the w-angle\n",
    "    data[\"Angle_w\"] = 90-data[\"Angle_a\"]\n",
    "\n",
    "    # Calculate windsupport\n",
    "    # Formula from https://doi.org/10.1186/2051-3933-1-4\n",
    "    # Windsupport is in the direction of the heading\n",
    "    data[\"WindSupport_PL\"] = data[\"WindSpeed_PL\"]*np.sin(data[\"Angle_w\"]*np.pi/180)\n",
    "    \n",
    "    #-----------------#\n",
    "    #- Save the data -#\n",
    "    #-----------------#\n",
    "    \n",
    "    # Save the data into a new file\n",
    "    data.to_pickle(data_folder+\"TestWindDiff/TestWindDiff_All.pkl\")\n",
    "    data.to_csv(data_folder+\"TestWindDiff/TestWindDiff_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateWind()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### General temperature pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeTempData():\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"TestTempDiff/TestTempDiff_\"+(\"[0-9]\"*4)+\"_stork\"+(\"[0-9]\"*4)+\".csv-\"+\"*\"+\".csv\")\n",
    "    \n",
    "    # Load the data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new file\n",
    "    data.to_pickle(data_folder+\"TestTempDiff/TestTempDiff_All.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "MergeTempData()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcAvgTemp():\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+\"TestTempDiff/TestTempDiff_All.pkl\")\n",
    "    \n",
    "    # Transform timestamps and add the date without time\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    data[\"Day\"] = data[\"timestamp\"].dt.date\n",
    "    \n",
    "    # Calculate average temperatures per day\n",
    "    data2 = data.groupby([\"Day\",\"Year\"],as_index=False).agg(\n",
    "        Temp = (\"ECMWF ERA5 SL Temperature (2 m above Ground)\",'mean'),\n",
    "        FTS = (\"timestamp\",'first')\n",
    "    )\n",
    "    \n",
    "    # Add day of the year\n",
    "    data2[\"YD\"] = data2[\"FTS\"].dt.strftime(\"%j\").astype(int)\n",
    "    \n",
    "    # Save the data into a new file\n",
    "    data.to_csv(data_folder+\"TestTempDiff/TestTempDiff_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalcAvgTemp()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General pattern boundary layer height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeBLHData():\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"TestBLHDiff/TestBLHDiff_\"+(\"[0-9]\"*4)+\"_stork\"+(\"[0-9]\"*4)+\".csv-\"+\"*\"+\".csv\")\n",
    "    \n",
    "    # Load the data files\n",
    "    data_all = (pd.read_csv(f,sep=',',low_memory=False) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new file\n",
    "    data.to_pickle(data_folder+\"TestBLHDiff/TestBLHDiff_All.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "MergeBLHData()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcAvgBLH():\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+\"TestBLHDiff/TestBLHDiff_All.pkl\")\n",
    "    \n",
    "    # Transform timestamps and add the date without time\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    data[\"Day\"] = data[\"timestamp\"].dt.date\n",
    "    \n",
    "    # Calculate average temperatures per day\n",
    "    data2 = data.groupby([\"Day\",\"Year\"],as_index=False).agg(\n",
    "        BLH = (\"ECMWF ERA5 SL Boundary Layer Height\",'mean'),\n",
    "        FTS = (\"timestamp\",'first')\n",
    "    )\n",
    "    \n",
    "    # Add day of the year\n",
    "    data2[\"YD\"] = data2[\"FTS\"].dt.strftime(\"%j\").astype(int)\n",
    "    \n",
    "    # Save the data into a new file\n",
    "    data.to_csv(data_folder+\"TestBLHDiff/TestBLHDiff_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalcAvgBLH()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
